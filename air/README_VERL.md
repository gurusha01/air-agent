# veRL + MLGym Integration

This module enables multi-step RL training on MLGym tasks using veRL's GRPO algorithm, without manual server management.

## Architecture

```
┌─────────────────────────────────────┐
│   veRL GRPO Trainer                 │
│   - Manages vLLM server             │
│   - Handles weight updates          │
│   - Runs PPO optimization           │
└──────────────┬──────────────────────┘
               │
               ▼
┌─────────────────────────────────────┐
│   Rollout Worker                    │
│   - Calls server_manager.generate() │
│   - Manages MLGymEnv                │
│   - Builds token sequences + masks  │
└──────────────┬──────────────────────┘
               │
               ▼
┌─────────────────────────────────────┐
│   MLGymEnv                          │
│   - Docker container execution      │
│   - Returns observations            │
│   - Evaluates final score           │
└─────────────────────────────────────┘
```

## Key Concepts

### 1. Token Sequences with Masks

Instead of storing individual steps, we build **one continuous token sequence** per episode:

```
[PROMPT] [ACTION_1] [OBS_1] [ACTION_2] [OBS_2] ... [ACTION_N] [OBS_N]
   ↑         ↑          ↑         ↑         ↑          ↑         ↑
  (init)   mask=1    mask=0    mask=1    mask=0    mask=1    mask=0
```

- **mask=1**: Tokens generated by LLM (actions) - receive policy gradients
- **mask=0**: Tokens from environment (observations) - no gradients

This allows veRL to train on multi-step trajectories while only updating the policy for LLM-generated tokens.

### 2. GRPO Group Sampling

GRPO requires **G trajectories** from the same initial state:

1. Create environment with seed S
2. Collect trajectory 1 with temperature T1
3. Reset environment with **same seed S**
4. Collect trajectory 2 with temperature T2
5. Repeat for G trajectories
6. GRPO computes advantages within this group

### 3. Server Management

veRL's `ServerManager` handles:
- Starting vLLM server at initialization
- Routing generation requests to server
- **Automatically syncing weight updates** after each training step
- No manual server restart needed!

### 4. Token Budget

Must track token usage across the multi-step episode:

```python
max_response_tokens = 4096  # Budget for entire episode
tokens_used = 0

while tokens_used < max_response_tokens:
    remaining = max_response_tokens - tokens_used
    generation = server.generate(max_tokens=min(512, remaining))
    tokens_used += len(generation.token_ids)
```

## File Structure

```
air/
├── verl_orchestrator.py   # Main training loop
├── rollout.py             # Episode collection & sequence building
└── README_VERL.md         # This file
```

## Usage

```bash
# Install dependencies
pip install verl transformers gymnasium

# Set up MLGym
export MLGYM_CONFIG_ROOT="/path/to/MLGym/configs"
export MLGYM_WORKSPACE_PATH="/path/to/MLGym/workspace"

# Run training
python -m air.verl_orchestrator
```

## Configuration

Edit `verl_orchestrator.py` to configure:

```python
# Task
task = "battleOfSexes"  # Any MLGym task

# Model
base_model = "Qwen/Qwen3-4B-Instruct-2507"

# Training
num_iterations = 10
rollouts_per_iteration = 16  # Must be divisible by group_size
group_size = 4  # G for GRPO

# Resources
vllm_gpu_ids = "0,1"  # GPUs for inference server
trainer_gpu_ids = "2,3,4,5,6,7"  # GPUs for training
```

## Key Functions

### `collect_episode_as_sequence()`
Collects one episode as a token sequence:

**Input:**
- `server_manager`: veRL's inference backend
- `tokenizer`: Model tokenizer
- `env`: MLGymEnv instance
- `agent_config`: Prompt templates
- `max_steps`: Episode length limit

**Output:**
```python
{
    "prompt_ids": [101, 2023, ...],      # Initial prompt tokens
    "response_ids": [1234, 5678, ...],   # All actions + observations
    "response_mask": [1, 1, 0, 0, ...],  # 1=LLM, 0=env
    "reward": 0.85,                       # Final score
    "num_steps": 23,                      # Steps taken
}
```

### `collect_grpo_group()`
Collects G trajectories from same initial state:

**Process:**
1. Generate random seed
2. For i in range(group_size):
   - Create env with **same seed**
   - Collect trajectory with temperature variation
   - Store trajectory
3. Return list of G trajectories

### `main()` Training Loop
```python
for iteration in range(num_iterations):
    # 1. Collect rollouts in groups
    all_trajectories = []
    for group in range(num_groups):
        group_trajs = collect_grpo_group(...)
        all_trajectories.extend(group_trajs)
    
    # 2. Format batch
    batch = {
        "prompt_ids": [...],
        "response_ids": [...],
        "response_mask": [...],
        "rewards": [...],
    }
    
    # 3. Training step (veRL handles everything)
    stats = trainer.step(batch)
    
    # 4. Save checkpoint
    trainer.save_checkpoint(...)
    
    # Server weights auto-updated! Next iteration uses new policy.
```

## How It Works

### Step-by-Step Walkthrough

1. **Initialization**
   ```python
   trainer = GRPOTrainer(config)
   # veRL starts vLLM server internally
   ```

2. **Episode Collection**
   ```python
   env = gym.make("mlgym/battleOfSexes")
   obs, info = env.reset()
   
   # Build initial prompt
   prompt = f"System: ...\nTask: ...\nOBSERVATION: {obs}\n\n"
   prompt_ids = tokenizer.encode(prompt)
   
   response_ids = []
   response_mask = []
   ```

3. **Multi-Step Interaction**
   ```python
   while not done:
       # Generate action via veRL server
       context = tokenizer.decode(prompt_ids + response_ids)
       output = server_manager.generate([context])[0]
       
       # Add LLM tokens (mask=1)
       action_tokens = tokenizer.encode(output.text)
       response_ids.extend(action_tokens)
       response_mask.extend([1] * len(action_tokens))
       
       # Execute in environment
       action = parse_action(output.text)
       obs, _, done, _ = env.step(action)
       
       # Add environment tokens (mask=0)
       obs_tokens = tokenizer.encode(f"\nOBSERVATION: {obs}\n")
       response_ids.extend(obs_tokens)
       response_mask.extend([0] * len(obs_tokens))
   ```

4. **Training**
   ```python
   # veRL computes advantages within groups
   # Applies PPO updates only to mask=1 tokens
   # Syncs new weights to vLLM server
   trainer.step(batch)
   ```

5. **Next Iteration**
   - Server already has updated weights
   - No restart needed
   - Just start collecting new rollouts

## Advantages

✅ **No manual server management** - veRL handles lifecycle  
✅ **Automatic weight updates** - Server syncs after each step  
✅ **True multi-step RL** - Full episode trajectories  
✅ **Efficient training** - Only gradients on LLM tokens  
✅ **Parallel rollouts** - Multiple envs on different GPUs  
✅ **Standard interface** - Works with any MLGym task  

## Troubleshooting

### "Out of memory during generation"
- Reduce `vllm_max_model_len`
- Reduce `max_response_tokens` per episode
- Use fewer `vllm_gpu_ids`

### "Server connection timeout"
- Increase vLLM startup wait time
- Check GPU availability: `nvidia-smi`

### "Low training rewards"
- Add reward shaping (per-step progress signals)
- Increase `group_size` for better advantage estimates
- Tune `kl_coef` (lower = more exploration)

### "Token budget exceeded"
- Reduce `max_response_tokens`
- Add stricter per-step `max_tokens` limit
- Use shorter observations

## Next Steps

1. **Test on simple task**: Start with `battleOfSexes`
2. **Monitor rewards**: Should improve over iterations
3. **Scale up**: Increase `rollouts_per_iteration`
4. **Experiment**: Try different tasks, hyperparameters
5. **Add logging**: Track per-step metrics, trajectory lengths

## References

- [veRL Documentation](https://github.com/volcengine/verl)
- [MLGym Repository](https://github.com/facebookresearch/MLGym)
- [GRPO Paper](https://arxiv.org/abs/2402.03300)

