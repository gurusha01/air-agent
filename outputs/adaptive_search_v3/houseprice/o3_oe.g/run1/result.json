{
  "task": "House Price Prediction (Kaggle)",
  "primary_metric": "r2",
  "higher_is_better": true,
  "selection_strategy": "open-ended",
  "best_node_id": "root",
  "best_score": 0.8799891474739243,
  "baseline_score": 0.8799891474739243,
  "improvement": 0.0,
  "total_nodes": 13,
  "elapsed_seconds": 10948.5,
  "tree_shape": {
    "root": {
      "depth": 0,
      "num_children": 12,
      "score": 0.8799891474739243,
      "strategy": "Baseline (no model execution)"
    },
    "root_0": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Train a LightGBM regressor with extensive feature engineering: (1) apply a Yeo-Johnson or log1p tran"
    },
    "root_1": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Build a two\u2013level stacking ensemble: (1) train diverse first-stage models\u2014ElasticNet on log-transfor"
    },
    "root_2": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Train a single CatBoostRegressor that natively handles the ~40 categorical columns, leaving them in "
    },
    "root_3": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Variation #1 \u2013 LightGBM \u201cdeep-leaf + noise\u201d tweak  \nStay with a single LightGBM model but (a) increa"
    },
    "root_4": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Variation: Keep the same LightGBM model but (1) raise max_depth from -1 (no limit) to 8, (2) lower l"
    },
    "root_5": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Baseline recap:\n\u2022 Pipeline: simple preprocessing (median imputation, one-hot encode categoricals) \u2192 "
    },
    "root_6": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "LightGBM \u201cslow-learn, many trees\u201d refinement  \nStep-by-step  \n1. Keep the same LightGBM model but:  "
    },
    "root_7": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Variation: Seed-bagged XGBoost with slightly lower learning rate and heavier column/row subsampling "
    },
    "root_8": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "LightGBM-based baseline tweak: \n\u2022 Keep the same LightGBM learner but (a) apply a log1p transformatio"
    },
    "root_9": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Refine the current XGBoost-based pipeline by adding two focused tweaks:\n\n1. Extended skew handling  "
    },
    "root_10": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Variation: Keep the current LightGBM-based pipeline but tighten the preprocessing and regularization"
    },
    "root_11": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Variation: keep the LightGBM model but introduce leave-one-out target encoding (with Gaussian smooth"
    }
  },
  "nodes": {
    "root": {
      "node_id": "root",
      "parent_id": null,
      "depth": 0,
      "strategy": "Baseline (no model execution)",
      "score": 0.8799891474739243,
      "actions_count": 0,
      "children": [
        "root_0",
        "root_1",
        "root_2",
        "root_3",
        "root_4",
        "root_5",
        "root_6",
        "root_7",
        "root_8",
        "root_9",
        "root_10",
        "root_11"
      ],
      "error": null
    },
    "root_0": {
      "node_id": "root_0",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Train a LightGBM regressor with extensive feature engineering: (1) apply a Yeo-Johnson or log1p tran",
      "score": null,
      "actions_count": 16,
      "children": [],
      "error": null
    },
    "root_1": {
      "node_id": "root_1",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Build a two\u2013level stacking ensemble: (1) train diverse first-stage models\u2014ElasticNet on log-transfor",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 795603 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
    },
    "root_2": {
      "node_id": "root_2",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Train a single CatBoostRegressor that natively handles the ~40 categorical columns, leaving them in ",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 794151 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
    },
    "root_3": {
      "node_id": "root_3",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Variation #1 \u2013 LightGBM \u201cdeep-leaf + noise\u201d tweak  \nStay with a single LightGBM model but (a) increa",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 794211 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
    },
    "root_4": {
      "node_id": "root_4",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Variation: Keep the same LightGBM model but (1) raise max_depth from -1 (no limit) to 8, (2) lower l",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 793662 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
    },
    "root_5": {
      "node_id": "root_5",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Baseline recap:\n\u2022 Pipeline: simple preprocessing (median imputation, one-hot encode categoricals) \u2192 ",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 795187 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
    },
    "root_6": {
      "node_id": "root_6",
      "parent_id": "root",
      "depth": 1,
      "strategy": "LightGBM \u201cslow-learn, many trees\u201d refinement  \nStep-by-step  \n1. Keep the same LightGBM model but:  ",
      "score": null,
      "actions_count": 16,
      "children": [],
      "error": null
    },
    "root_7": {
      "node_id": "root_7",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Variation: Seed-bagged XGBoost with slightly lower learning rate and heavier column/row subsampling ",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 793133 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
    },
    "root_8": {
      "node_id": "root_8",
      "parent_id": "root",
      "depth": 1,
      "strategy": "LightGBM-based baseline tweak: \n\u2022 Keep the same LightGBM learner but (a) apply a log1p transformatio",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 794219 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
    },
    "root_9": {
      "node_id": "root_9",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Refine the current XGBoost-based pipeline by adding two focused tweaks:\n\n1. Extended skew handling  ",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 793500 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
    },
    "root_10": {
      "node_id": "root_10",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Variation: Keep the current LightGBM-based pipeline but tighten the preprocessing and regularization",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 795798 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
    },
    "root_11": {
      "node_id": "root_11",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Variation: keep the LightGBM model but introduce leave-one-out target encoding (with Gaussian smooth",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 794622 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
    }
  }
}