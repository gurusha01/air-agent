{
  "task": "Titanic Survival Prediction",
  "primary_metric": "accuracy",
  "higher_is_better": true,
  "selection_strategy": "open-ended",
  "best_node_id": "root_1_1_0_0",
  "best_score": 0.9473684210526315,
  "baseline_score": 0.76555,
  "improvement": 0.18181842105263157,
  "total_nodes": 61,
  "elapsed_seconds": 18266.7,
  "tree_shape": {
    "root": {
      "depth": 0,
      "num_children": 3,
      "score": 0.76555,
      "strategy": "Baseline (no model execution)"
    },
    "root_0": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Apply a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) with targeted feature interaction"
    },
    "root_1": {
      "depth": 1,
      "num_children": 2,
      "score": 0.5861244019138756,
      "strategy": "Apply a gradient boosting ensemble (e.g., XGBoost or LightGBM) with feature interactions and target "
    },
    "root_2": {
      "depth": 1,
      "num_children": 0,
      "score": null,
      "strategy": "Apply a gradient boosting model (e.g., XGBoost or LightGBM) with targeted feature interactions, such"
    },
    "root_1_0": {
      "depth": 2,
      "num_children": 2,
      "score": 0.631578947368421,
      "strategy": "Replace the gradient boosting model with a neural network architecture using deep learning (e.g., a "
    },
    "root_1_0_0": {
      "depth": 3,
      "num_children": 2,
      "score": 0.6483253588516746,
      "strategy": "Replace the neural network with a stacked autoencoder (autoencoder-based feature learning) to perfor"
    },
    "root_1_0_0_0": {
      "depth": 4,
      "num_children": 2,
      "score": 0.5741626794258373,
      "strategy": "Apply a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) with target encoding for categori"
    },
    "root_1_0_0_0_0": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the gradient-boosted tree ensemble with a neural network (e.g., a deep multilayer perceptron"
    },
    "root_1_0_1": {
      "depth": 3,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the neural network with a random forest ensemble with bootstrap aggregating (bagging) and in"
    },
    "root_1_1": {
      "depth": 2,
      "num_children": 3,
      "score": 0.7918660287081339,
      "strategy": "Apply a neural network with deep architecture (e.g., 3-5 hidden layers with ReLU activations) using "
    },
    "root_1_1_0": {
      "depth": 3,
      "num_children": 3,
      "score": 0.8588516746411483,
      "strategy": "Replace the neural network with a gradient boosting ensemble (e.g., XGBoost or LightGBM) using tree-"
    },
    "root_1_1_0_0": {
      "depth": 4,
      "num_children": 3,
      "score": 0.9473684210526315,
      "strategy": "Introduce a deep autoencoder-based feature learning framework to auto-extract latent representations"
    },
    "root_1_1_0_0_0": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Introduce a graph neural network (GNN) to model passenger relationships based on shared cabin, famil"
    },
    "root_1_1_0_0_1": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Introduce a graph neural network (GNN) framework where passengers are represented as nodes in a soci"
    },
    "root_1_1_0_1": {
      "depth": 4,
      "num_children": 3,
      "score": 0.9473684210526315,
      "strategy": "Introduce a deep autoencoder-based feature learning approach to capture latent representations of th"
    },
    "root_1_1_0_1_0": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the deep autoencoder with a contrastive learning framework (e.g., SimCLR or BYOL) applied to"
    },
    "root_1_1_0_1_1": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the autoencoder-based latent space with a contrastive learning framework (e.g., SimCLR or Mo"
    },
    "root_1_1_1": {
      "depth": 3,
      "num_children": 2,
      "score": 0.8421052631578947,
      "strategy": "Replace the neural network with a gradient boosting ensemble (e.g., XGBoost or LightGBM) using tree-"
    },
    "root_1_1_1_0": {
      "depth": 4,
      "num_children": 0,
      "score": null,
      "strategy": "Apply a random forest with bootstrap aggregating (bagging) and introduce synthetic features via poly"
    },
    "root_1_1_1_1": {
      "depth": 4,
      "num_children": 2,
      "score": 0.8205741626794258,
      "strategy": "Apply a random forest with permutation importance-based feature selection to identify the most predi"
    },
    "root_1_1_1_1_0": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Apply a gradient-boosted model (e.g., XGBoost or LightGBM) with target encoding applied to categoric"
    },
    "root_1_1_1_1_1": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the random forest with a gradient-boosting model (e.g., XGBoost or LightGBM) and apply targe"
    },
    "root_1_1_0_0_2": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Introduce a graph neural network (GNN) framework where passengers are represented as nodes in a soci"
    },
    "root_1_1_0_1_2": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the deep autoencoder with a Transformer-based architecture that processes tabular data using"
    },
    "root_1_1_0_2": {
      "depth": 4,
      "num_children": 2,
      "score": 0.6363636363636364,
      "strategy": "Introduce a deep autoencoder-based latent space representation of the passenger features to capture "
    },
    "root_1_1_0_2_0": {
      "depth": 5,
      "num_children": 2,
      "score": 0.8157894736842105,
      "strategy": "Replace the deep autoencoder with a random forest model trained on raw passenger features (e.g., age"
    },
    "root_1_1_0_2_0_0": {
      "depth": 6,
      "num_children": 0,
      "score": null,
      "strategy": "Introduce a gradient boosting model (e.g., XGBoost or LightGBM) trained on raw passenger features wi"
    },
    "root_1_1_0_2_0_1": {
      "depth": 6,
      "num_children": 0,
      "score": null,
      "strategy": "Introduce a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) with target encoding applied "
    },
    "root_1_1_0_2_1": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Apply a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) with feature interactions explici"
    },
    "root_1_1_2": {
      "depth": 3,
      "num_children": 2,
      "score": 0.638755980861244,
      "strategy": "Replace the neural network with a gradient boosting ensemble (e.g., XGBoost or LightGBM) using tree-"
    },
    "root_1_1_2_0": {
      "depth": 4,
      "num_children": 2,
      "score": 0.6435406698564593,
      "strategy": "Apply a deep autoencoder-based feature learning approach to reduce dimensionality and extract latent"
    },
    "root_1_1_2_0_0": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Apply a random forest ensemble with gradient-based feature importance pruning to identify and weight"
    },
    "root_1_1_2_1": {
      "depth": 4,
      "num_children": 2,
      "score": 0.6244019138755981,
      "strategy": "Apply a deep autoencoder-based unsupervised feature learning approach to reduce dimensionality and e"
    },
    "root_1_1_2_1_0": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the deep autoencoder with a transformer-based self-attention model applied to tabular data ("
    },
    "root_1_1_2_0_1": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Introduce a hybrid model combining gradient boosting (e.g., XGBoost or LightGBM) with a small-scale "
    },
    "root_1_1_2_1_1": {
      "depth": 5,
      "num_children": 2,
      "score": 0.8349282296650717,
      "strategy": "Replace the deep autoencoder with a transformer-based self-attention model applied to tabular data ("
    },
    "root_1_1_2_1_1_0": {
      "depth": 6,
      "num_children": 0,
      "score": null,
      "strategy": "Apply a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) directly on the raw features with"
    },
    "root_1_1_2_1_1_1": {
      "depth": 6,
      "num_children": 0,
      "score": null,
      "strategy": "Apply a gradient-boosted tree ensemble (e.g., XGBoost, LightGBM, or CatBoost) directly on the raw fe"
    },
    "root_1_0_0_1": {
      "depth": 4,
      "num_children": 2,
      "score": 0.5789473684210527,
      "strategy": "Apply gradient boosting with feature interactions derived from pairwise correlations between continu"
    },
    "root_1_0_0_1_0": {
      "depth": 5,
      "num_children": 2,
      "score": 0.6435406698564593,
      "strategy": "Apply a random forest with permutation importance to identify and prioritize the most predictive fea"
    },
    "root_1_0_0_1_0_0": {
      "depth": 6,
      "num_children": 3,
      "score": 0.861244019138756,
      "strategy": "Replace the random forest with a gradient boosting machine (e.g., XGBoost or LightGBM) and incorpora"
    },
    "root_1_0_0_1_0_0_0": {
      "depth": 7,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the gradient boosting model with a neural network architecture using a multilayer perceptron"
    },
    "root_1_0_0_1_0_0_1": {
      "depth": 7,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the gradient boosting model with a neural network architecture using a deep feedforward netw"
    },
    "root_1_0_0_1_0_1": {
      "depth": 6,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the random forest with a gradient boosting machine (e.g., XGBoost or LightGBM) and apply tar"
    },
    "root_1_0_0_1_1": {
      "depth": 5,
      "num_children": 2,
      "score": 0.5861244019138756,
      "strategy": "Replace gradient boosting with a neural network architecture that includes deep feature learning usi"
    },
    "root_1_0_0_1_1_0": {
      "depth": 6,
      "num_children": 2,
      "score": 0.6339712918660287,
      "strategy": "Introduce a random forest with permutation importance-based feature selection to identify the most p"
    },
    "root_1_0_0_1_1_0_0": {
      "depth": 7,
      "num_children": 2,
      "score": 0.6076555023923444,
      "strategy": "Replace the gradient-boosted model with a neural network architecture incorporating attention mechan"
    },
    "root_1_0_0_1_1_0_0_0": {
      "depth": 8,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the neural network with a random forest ensemble with gradient boosting (e.g., XGBoost or Li"
    },
    "root_1_0_0_1_1_1": {
      "depth": 6,
      "num_children": 2,
      "score": 0.5933014354066986,
      "strategy": "Introduce a graph neural network (GNN) to model passenger relationships based on categorical feature"
    },
    "root_1_0_0_1_1_1_0": {
      "depth": 7,
      "num_children": 2,
      "score": 0.5933014354066986,
      "strategy": "Introduce a time-series recurrent model (e.g., LSTM or GRU) to model passenger survival based on tem"
    },
    "root_1_0_0_1_1_1_0_0": {
      "depth": 8,
      "num_children": 0,
      "score": null,
      "strategy": "Introduce a graph neural network (GNN) with passenger relationships as nodes, where edges represent "
    },
    "root_1_0_0_1_1_0_1": {
      "depth": 7,
      "num_children": 2,
      "score": 0.854066985645933,
      "strategy": "Apply a neural network with attention mechanisms to model interactions between passenger features (e"
    },
    "root_1_0_0_1_1_0_1_0": {
      "depth": 8,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the neural network with a lightweight gradient-boosted decision tree ensemble (e.g., LightGB"
    },
    "root_1_0_0_1_1_0_1_1": {
      "depth": 8,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the neural network with a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) using t"
    },
    "root_1_0_0_1_1_0_0_1": {
      "depth": 8,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the neural network with a random forest ensemble where each tree is trained on a different b"
    },
    "root_1_0_0_1_1_1_1": {
      "depth": 7,
      "num_children": 2,
      "score": 0.8277511961722488,
      "strategy": "Introduce a time-series transformer model to capture temporal dependencies in passenger survival by "
    },
    "root_1_0_0_1_1_1_1_0": {
      "depth": 8,
      "num_children": 0,
      "score": null,
      "strategy": "Introduce a graph neural network (GNN) to model passenger relationships as a social network, where n"
    },
    "root_1_0_0_1_1_1_1_1": {
      "depth": 8,
      "num_children": 0,
      "score": null,
      "strategy": "Introduce a graph neural network (GNN) to model passenger relationships as a social network, where n"
    },
    "root_1_0_0_1_1_1_0_1": {
      "depth": 8,
      "num_children": 0,
      "score": null,
      "strategy": "Introduce a graph neural network (GNN) with passenger relationships as nodes, where edges represent "
    },
    "root_1_0_0_0_1": {
      "depth": 5,
      "num_children": 0,
      "score": null,
      "strategy": "Replace the gradient-boosted tree ensemble with a neural network architecture using a multi-layer pe"
    },
    "root_1_0_0_1_0_0_2": {
      "depth": 7,
      "num_children": 0,
      "score": 0.8492822966507177,
      "strategy": "Replace the gradient boosting model with a neural network architecture using a deep learning approac"
    }
  },
  "nodes": {
    "root": {
      "node_id": "root",
      "parent_id": null,
      "depth": 0,
      "strategy": "Baseline (no model execution)",
      "score": 0.76555,
      "actions_count": 0,
      "children": [
        "root_0",
        "root_1",
        "root_2"
      ],
      "error": null
    },
    "root_0": {
      "node_id": "root_0",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Apply a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) with targeted feature interaction",
      "score": null,
      "actions_count": 16,
      "children": [],
      "error": null
    },
    "root_1": {
      "node_id": "root_1",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Apply a gradient boosting ensemble (e.g., XGBoost or LightGBM) with feature interactions and target ",
      "score": 0.5861244019138756,
      "actions_count": 9,
      "children": [
        "root_1_0",
        "root_1_1"
      ],
      "error": null
    },
    "root_2": {
      "node_id": "root_2",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Apply a gradient boosting model (e.g., XGBoost or LightGBM) with targeted feature interactions, such",
      "score": null,
      "actions_count": 16,
      "children": [],
      "error": null
    },
    "root_1_0": {
      "node_id": "root_1_0",
      "parent_id": "root_1",
      "depth": 2,
      "strategy": "Replace the gradient boosting model with a neural network architecture using deep learning (e.g., a ",
      "score": 0.631578947368421,
      "actions_count": 12,
      "children": [
        "root_1_0_0",
        "root_1_0_1"
      ],
      "error": null
    },
    "root_1_0_0": {
      "node_id": "root_1_0_0",
      "parent_id": "root_1_0",
      "depth": 3,
      "strategy": "Replace the neural network with a stacked autoencoder (autoencoder-based feature learning) to perfor",
      "score": 0.6483253588516746,
      "actions_count": 7,
      "children": [
        "root_1_0_0_0",
        "root_1_0_0_1"
      ],
      "error": null
    },
    "root_1_0_0_0": {
      "node_id": "root_1_0_0_0",
      "parent_id": "root_1_0_0",
      "depth": 4,
      "strategy": "Apply a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) with target encoding for categori",
      "score": 0.5741626794258373,
      "actions_count": 11,
      "children": [
        "root_1_0_0_0_0",
        "root_1_0_0_0_1"
      ],
      "error": null
    },
    "root_1_0_0_0_0": {
      "node_id": "root_1_0_0_0_0",
      "parent_id": "root_1_0_0_0",
      "depth": 5,
      "strategy": "Replace the gradient-boosted tree ensemble with a neural network (e.g., a deep multilayer perceptron",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29164 input tokens (4096 > 32768 - 29164). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_1": {
      "node_id": "root_1_0_1",
      "parent_id": "root_1_0",
      "depth": 3,
      "strategy": "Replace the neural network with a random forest ensemble with bootstrap aggregating (bagging) and in",
      "score": null,
      "actions_count": 16,
      "children": [],
      "error": null
    },
    "root_1_1": {
      "node_id": "root_1_1",
      "parent_id": "root_1",
      "depth": 2,
      "strategy": "Apply a neural network with deep architecture (e.g., 3-5 hidden layers with ReLU activations) using ",
      "score": 0.7918660287081339,
      "actions_count": 10,
      "children": [
        "root_1_1_0",
        "root_1_1_1",
        "root_1_1_2"
      ],
      "error": null
    },
    "root_1_1_0": {
      "node_id": "root_1_1_0",
      "parent_id": "root_1_1",
      "depth": 3,
      "strategy": "Replace the neural network with a gradient boosting ensemble (e.g., XGBoost or LightGBM) using tree-",
      "score": 0.8588516746411483,
      "actions_count": 8,
      "children": [
        "root_1_1_0_0",
        "root_1_1_0_1",
        "root_1_1_0_2"
      ],
      "error": null
    },
    "root_1_1_0_0": {
      "node_id": "root_1_1_0_0",
      "parent_id": "root_1_1_0",
      "depth": 4,
      "strategy": "Introduce a deep autoencoder-based feature learning framework to auto-extract latent representations",
      "score": 0.9473684210526315,
      "actions_count": 7,
      "children": [
        "root_1_1_0_0_0",
        "root_1_1_0_0_1",
        "root_1_1_0_0_2"
      ],
      "error": null
    },
    "root_1_1_0_0_0": {
      "node_id": "root_1_1_0_0_0",
      "parent_id": "root_1_1_0_0",
      "depth": 5,
      "strategy": "Introduce a graph neural network (GNN) to model passenger relationships based on shared cabin, famil",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29058 input tokens (4096 > 32768 - 29058). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_0_0_1": {
      "node_id": "root_1_1_0_0_1",
      "parent_id": "root_1_1_0_0",
      "depth": 5,
      "strategy": "Introduce a graph neural network (GNN) framework where passengers are represented as nodes in a soci",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29191 input tokens (4096 > 32768 - 29191). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_0_1": {
      "node_id": "root_1_1_0_1",
      "parent_id": "root_1_1_0",
      "depth": 4,
      "strategy": "Introduce a deep autoencoder-based feature learning approach to capture latent representations of th",
      "score": 0.9473684210526315,
      "actions_count": 8,
      "children": [
        "root_1_1_0_1_0",
        "root_1_1_0_1_1",
        "root_1_1_0_1_2"
      ],
      "error": null
    },
    "root_1_1_0_1_0": {
      "node_id": "root_1_1_0_1_0",
      "parent_id": "root_1_1_0_1",
      "depth": 5,
      "strategy": "Replace the deep autoencoder with a contrastive learning framework (e.g., SimCLR or BYOL) applied to",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29034 input tokens (4096 > 32768 - 29034). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_0_1_1": {
      "node_id": "root_1_1_0_1_1",
      "parent_id": "root_1_1_0_1",
      "depth": 5,
      "strategy": "Replace the autoencoder-based latent space with a contrastive learning framework (e.g., SimCLR or Mo",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28683 input tokens (4096 > 32768 - 28683). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_1": {
      "node_id": "root_1_1_1",
      "parent_id": "root_1_1",
      "depth": 3,
      "strategy": "Replace the neural network with a gradient boosting ensemble (e.g., XGBoost or LightGBM) using tree-",
      "score": 0.8421052631578947,
      "actions_count": 8,
      "children": [
        "root_1_1_1_0",
        "root_1_1_1_1"
      ],
      "error": null
    },
    "root_1_1_1_0": {
      "node_id": "root_1_1_1_0",
      "parent_id": "root_1_1_1",
      "depth": 4,
      "strategy": "Apply a random forest with bootstrap aggregating (bagging) and introduce synthetic features via poly",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28793 input tokens (4096 > 32768 - 28793). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_1_1": {
      "node_id": "root_1_1_1_1",
      "parent_id": "root_1_1_1",
      "depth": 4,
      "strategy": "Apply a random forest with permutation importance-based feature selection to identify the most predi",
      "score": 0.8205741626794258,
      "actions_count": 3,
      "children": [
        "root_1_1_1_1_0",
        "root_1_1_1_1_1"
      ],
      "error": null
    },
    "root_1_1_1_1_0": {
      "node_id": "root_1_1_1_1_0",
      "parent_id": "root_1_1_1_1",
      "depth": 5,
      "strategy": "Apply a gradient-boosted model (e.g., XGBoost or LightGBM) with target encoding applied to categoric",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28917 input tokens (4096 > 32768 - 28917). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_1_1_1": {
      "node_id": "root_1_1_1_1_1",
      "parent_id": "root_1_1_1_1",
      "depth": 5,
      "strategy": "Replace the random forest with a gradient-boosting model (e.g., XGBoost or LightGBM) and apply targe",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29557 input tokens (4096 > 32768 - 29557). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_0_0_2": {
      "node_id": "root_1_1_0_0_2",
      "parent_id": "root_1_1_0_0",
      "depth": 5,
      "strategy": "Introduce a graph neural network (GNN) framework where passengers are represented as nodes in a soci",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29163 input tokens (4096 > 32768 - 29163). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_0_1_2": {
      "node_id": "root_1_1_0_1_2",
      "parent_id": "root_1_1_0_1",
      "depth": 5,
      "strategy": "Replace the deep autoencoder with a Transformer-based architecture that processes tabular data using",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29205 input tokens (4096 > 32768 - 29205). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_0_2": {
      "node_id": "root_1_1_0_2",
      "parent_id": "root_1_1_0",
      "depth": 4,
      "strategy": "Introduce a deep autoencoder-based latent space representation of the passenger features to capture ",
      "score": 0.6363636363636364,
      "actions_count": 5,
      "children": [
        "root_1_1_0_2_0",
        "root_1_1_0_2_1"
      ],
      "error": null
    },
    "root_1_1_0_2_0": {
      "node_id": "root_1_1_0_2_0",
      "parent_id": "root_1_1_0_2",
      "depth": 5,
      "strategy": "Replace the deep autoencoder with a random forest model trained on raw passenger features (e.g., age",
      "score": 0.8157894736842105,
      "actions_count": 3,
      "children": [
        "root_1_1_0_2_0_0",
        "root_1_1_0_2_0_1"
      ],
      "error": null
    },
    "root_1_1_0_2_0_0": {
      "node_id": "root_1_1_0_2_0_0",
      "parent_id": "root_1_1_0_2_0",
      "depth": 6,
      "strategy": "Introduce a gradient boosting model (e.g., XGBoost or LightGBM) trained on raw passenger features wi",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29714 input tokens (4096 > 32768 - 29714). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_0_2_0_1": {
      "node_id": "root_1_1_0_2_0_1",
      "parent_id": "root_1_1_0_2_0",
      "depth": 6,
      "strategy": "Introduce a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) with target encoding applied ",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29763 input tokens (4096 > 32768 - 29763). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_0_2_1": {
      "node_id": "root_1_1_0_2_1",
      "parent_id": "root_1_1_0_2",
      "depth": 5,
      "strategy": "Apply a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) with feature interactions explici",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29020 input tokens (4096 > 32768 - 29020). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_2": {
      "node_id": "root_1_1_2",
      "parent_id": "root_1_1",
      "depth": 3,
      "strategy": "Replace the neural network with a gradient boosting ensemble (e.g., XGBoost or LightGBM) using tree-",
      "score": 0.638755980861244,
      "actions_count": 9,
      "children": [
        "root_1_1_2_0",
        "root_1_1_2_1"
      ],
      "error": null
    },
    "root_1_1_2_0": {
      "node_id": "root_1_1_2_0",
      "parent_id": "root_1_1_2",
      "depth": 4,
      "strategy": "Apply a deep autoencoder-based feature learning approach to reduce dimensionality and extract latent",
      "score": 0.6435406698564593,
      "actions_count": 3,
      "children": [
        "root_1_1_2_0_0",
        "root_1_1_2_0_1"
      ],
      "error": null
    },
    "root_1_1_2_0_0": {
      "node_id": "root_1_1_2_0_0",
      "parent_id": "root_1_1_2_0",
      "depth": 5,
      "strategy": "Apply a random forest ensemble with gradient-based feature importance pruning to identify and weight",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29630 input tokens (4096 > 32768 - 29630). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_2_1": {
      "node_id": "root_1_1_2_1",
      "parent_id": "root_1_1_2",
      "depth": 4,
      "strategy": "Apply a deep autoencoder-based unsupervised feature learning approach to reduce dimensionality and e",
      "score": 0.6244019138755981,
      "actions_count": 6,
      "children": [
        "root_1_1_2_1_0",
        "root_1_1_2_1_1"
      ],
      "error": null
    },
    "root_1_1_2_1_0": {
      "node_id": "root_1_1_2_1_0",
      "parent_id": "root_1_1_2_1",
      "depth": 5,
      "strategy": "Replace the deep autoencoder with a transformer-based self-attention model applied to tabular data (",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29043 input tokens (4096 > 32768 - 29043). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_2_0_1": {
      "node_id": "root_1_1_2_0_1",
      "parent_id": "root_1_1_2_0",
      "depth": 5,
      "strategy": "Introduce a hybrid model combining gradient boosting (e.g., XGBoost or LightGBM) with a small-scale ",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29214 input tokens (4096 > 32768 - 29214). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_2_1_1": {
      "node_id": "root_1_1_2_1_1",
      "parent_id": "root_1_1_2_1",
      "depth": 5,
      "strategy": "Replace the deep autoencoder with a transformer-based self-attention model applied to tabular data (",
      "score": 0.8349282296650717,
      "actions_count": 3,
      "children": [
        "root_1_1_2_1_1_0",
        "root_1_1_2_1_1_1"
      ],
      "error": null
    },
    "root_1_1_2_1_1_0": {
      "node_id": "root_1_1_2_1_1_0",
      "parent_id": "root_1_1_2_1_1",
      "depth": 6,
      "strategy": "Apply a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) directly on the raw features with",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29730 input tokens (4096 > 32768 - 29730). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_1_2_1_1_1": {
      "node_id": "root_1_1_2_1_1_1",
      "parent_id": "root_1_1_2_1_1",
      "depth": 6,
      "strategy": "Apply a gradient-boosted tree ensemble (e.g., XGBoost, LightGBM, or CatBoost) directly on the raw fe",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29738 input tokens (4096 > 32768 - 29738). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1": {
      "node_id": "root_1_0_0_1",
      "parent_id": "root_1_0_0",
      "depth": 4,
      "strategy": "Apply gradient boosting with feature interactions derived from pairwise correlations between continu",
      "score": 0.5789473684210527,
      "actions_count": 3,
      "children": [
        "root_1_0_0_1_0",
        "root_1_0_0_1_1"
      ],
      "error": null
    },
    "root_1_0_0_1_0": {
      "node_id": "root_1_0_0_1_0",
      "parent_id": "root_1_0_0_1",
      "depth": 5,
      "strategy": "Apply a random forest with permutation importance to identify and prioritize the most predictive fea",
      "score": 0.6435406698564593,
      "actions_count": 3,
      "children": [
        "root_1_0_0_1_0_0",
        "root_1_0_0_1_0_1"
      ],
      "error": null
    },
    "root_1_0_0_1_0_0": {
      "node_id": "root_1_0_0_1_0_0",
      "parent_id": "root_1_0_0_1_0",
      "depth": 6,
      "strategy": "Replace the random forest with a gradient boosting machine (e.g., XGBoost or LightGBM) and incorpora",
      "score": 0.861244019138756,
      "actions_count": 9,
      "children": [
        "root_1_0_0_1_0_0_0",
        "root_1_0_0_1_0_0_1",
        "root_1_0_0_1_0_0_2"
      ],
      "error": null
    },
    "root_1_0_0_1_0_0_0": {
      "node_id": "root_1_0_0_1_0_0_0",
      "parent_id": "root_1_0_0_1_0_0",
      "depth": 7,
      "strategy": "Replace the gradient boosting model with a neural network architecture using a multilayer perceptron",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29150 input tokens (4096 > 32768 - 29150). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_0_0_1": {
      "node_id": "root_1_0_0_1_0_0_1",
      "parent_id": "root_1_0_0_1_0_0",
      "depth": 7,
      "strategy": "Replace the gradient boosting model with a neural network architecture using a deep feedforward netw",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28931 input tokens (4096 > 32768 - 28931). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_0_1": {
      "node_id": "root_1_0_0_1_0_1",
      "parent_id": "root_1_0_0_1_0",
      "depth": 6,
      "strategy": "Replace the random forest with a gradient boosting machine (e.g., XGBoost or LightGBM) and apply tar",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29038 input tokens (4096 > 32768 - 29038). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_1": {
      "node_id": "root_1_0_0_1_1",
      "parent_id": "root_1_0_0_1",
      "depth": 5,
      "strategy": "Replace gradient boosting with a neural network architecture that includes deep feature learning usi",
      "score": 0.5861244019138756,
      "actions_count": 7,
      "children": [
        "root_1_0_0_1_1_0",
        "root_1_0_0_1_1_1"
      ],
      "error": null
    },
    "root_1_0_0_1_1_0": {
      "node_id": "root_1_0_0_1_1_0",
      "parent_id": "root_1_0_0_1_1",
      "depth": 6,
      "strategy": "Introduce a random forest with permutation importance-based feature selection to identify the most p",
      "score": 0.6339712918660287,
      "actions_count": 5,
      "children": [
        "root_1_0_0_1_1_0_0",
        "root_1_0_0_1_1_0_1"
      ],
      "error": null
    },
    "root_1_0_0_1_1_0_0": {
      "node_id": "root_1_0_0_1_1_0_0",
      "parent_id": "root_1_0_0_1_1_0",
      "depth": 7,
      "strategy": "Replace the gradient-boosted model with a neural network architecture incorporating attention mechan",
      "score": 0.6076555023923444,
      "actions_count": 5,
      "children": [
        "root_1_0_0_1_1_0_0_0",
        "root_1_0_0_1_1_0_0_1"
      ],
      "error": null
    },
    "root_1_0_0_1_1_0_0_0": {
      "node_id": "root_1_0_0_1_1_0_0_0",
      "parent_id": "root_1_0_0_1_1_0_0",
      "depth": 8,
      "strategy": "Replace the neural network with a random forest ensemble with gradient boosting (e.g., XGBoost or Li",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29003 input tokens (4096 > 32768 - 29003). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_1_1": {
      "node_id": "root_1_0_0_1_1_1",
      "parent_id": "root_1_0_0_1_1",
      "depth": 6,
      "strategy": "Introduce a graph neural network (GNN) to model passenger relationships based on categorical feature",
      "score": 0.5933014354066986,
      "actions_count": 5,
      "children": [
        "root_1_0_0_1_1_1_0",
        "root_1_0_0_1_1_1_1"
      ],
      "error": null
    },
    "root_1_0_0_1_1_1_0": {
      "node_id": "root_1_0_0_1_1_1_0",
      "parent_id": "root_1_0_0_1_1_1",
      "depth": 7,
      "strategy": "Introduce a time-series recurrent model (e.g., LSTM or GRU) to model passenger survival based on tem",
      "score": 0.5933014354066986,
      "actions_count": 3,
      "children": [
        "root_1_0_0_1_1_1_0_0",
        "root_1_0_0_1_1_1_0_1"
      ],
      "error": null
    },
    "root_1_0_0_1_1_1_0_0": {
      "node_id": "root_1_0_0_1_1_1_0_0",
      "parent_id": "root_1_0_0_1_1_1_0",
      "depth": 8,
      "strategy": "Introduce a graph neural network (GNN) with passenger relationships as nodes, where edges represent ",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28737 input tokens (4096 > 32768 - 28737). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_1_0_1": {
      "node_id": "root_1_0_0_1_1_0_1",
      "parent_id": "root_1_0_0_1_1_0",
      "depth": 7,
      "strategy": "Apply a neural network with attention mechanisms to model interactions between passenger features (e",
      "score": 0.854066985645933,
      "actions_count": 5,
      "children": [
        "root_1_0_0_1_1_0_1_0",
        "root_1_0_0_1_1_0_1_1"
      ],
      "error": null
    },
    "root_1_0_0_1_1_0_1_0": {
      "node_id": "root_1_0_0_1_1_0_1_0",
      "parent_id": "root_1_0_0_1_1_0_1",
      "depth": 8,
      "strategy": "Replace the neural network with a lightweight gradient-boosted decision tree ensemble (e.g., LightGB",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29341 input tokens (4096 > 32768 - 29341). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_1_0_1_1": {
      "node_id": "root_1_0_0_1_1_0_1_1",
      "parent_id": "root_1_0_0_1_1_0_1",
      "depth": 8,
      "strategy": "Replace the neural network with a gradient-boosted tree ensemble (e.g., XGBoost or LightGBM) using t",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29328 input tokens (4096 > 32768 - 29328). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_1_0_0_1": {
      "node_id": "root_1_0_0_1_1_0_0_1",
      "parent_id": "root_1_0_0_1_1_0_0",
      "depth": 8,
      "strategy": "Replace the neural network with a random forest ensemble where each tree is trained on a different b",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29061 input tokens (4096 > 32768 - 29061). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_1_1_1": {
      "node_id": "root_1_0_0_1_1_1_1",
      "parent_id": "root_1_0_0_1_1_1",
      "depth": 7,
      "strategy": "Introduce a time-series transformer model to capture temporal dependencies in passenger survival by ",
      "score": 0.8277511961722488,
      "actions_count": 3,
      "children": [
        "root_1_0_0_1_1_1_1_0",
        "root_1_0_0_1_1_1_1_1"
      ],
      "error": null
    },
    "root_1_0_0_1_1_1_1_0": {
      "node_id": "root_1_0_0_1_1_1_1_0",
      "parent_id": "root_1_0_0_1_1_1_1",
      "depth": 8,
      "strategy": "Introduce a graph neural network (GNN) to model passenger relationships as a social network, where n",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28970 input tokens (4096 > 32768 - 28970). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_1_1_1_1": {
      "node_id": "root_1_0_0_1_1_1_1_1",
      "parent_id": "root_1_0_0_1_1_1_1",
      "depth": 8,
      "strategy": "Introduce a graph neural network (GNN) to model passenger relationships as a social network, where n",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28848 input tokens (4096 > 32768 - 28848). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_1_1_0_1": {
      "node_id": "root_1_0_0_1_1_1_0_1",
      "parent_id": "root_1_0_0_1_1_1_0",
      "depth": 8,
      "strategy": "Introduce a graph neural network (GNN) with passenger relationships as nodes, where edges represent ",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29037 input tokens (4096 > 32768 - 29037). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_0_1": {
      "node_id": "root_1_0_0_0_1",
      "parent_id": "root_1_0_0_0",
      "depth": 5,
      "strategy": "Replace the gradient-boosted tree ensemble with a neural network architecture using a multi-layer pe",
      "score": null,
      "actions_count": 0,
      "children": [],
      "error": "LLM failed: LLM failed after 3 attempts: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 29152 input tokens (4096 > 32768 - 29152). (parameter=max_tokens, value=4096) None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}"
    },
    "root_1_0_0_1_0_0_2": {
      "node_id": "root_1_0_0_1_0_0_2",
      "parent_id": "root_1_0_0_1_0_0",
      "depth": 7,
      "strategy": "Replace the gradient boosting model with a neural network architecture using a deep learning approac",
      "score": 0.8492822966507177,
      "actions_count": 5,
      "children": [],
      "error": null
    }
  }
}