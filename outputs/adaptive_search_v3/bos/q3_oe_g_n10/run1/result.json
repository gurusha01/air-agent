{
  "task": "Battle of Sexes",
  "primary_metric": "Score",
  "higher_is_better": true,
  "selection_strategy": "open-ended",
  "best_node_id": "root_0_0_0_0_0_0",
  "best_score": 1.2703601121902466,
  "baseline_score": 1.0226699113845825,
  "improvement": 0.24769020080566406,
  "total_nodes": 11,
  "elapsed_seconds": 133.8,
  "tree_shape": {
    "root": {
      "depth": 0,
      "num_children": 3,
      "score": 1.0226699113845825,
      "strategy": "Baseline (no model execution)"
    },
    "root_0": {
      "depth": 1,
      "num_children": 1,
      "score": 0.866890013217926,
      "strategy": "Use a mixed strategy where the row player randomizes between coordinating with the column player and"
    },
    "root_1": {
      "depth": 1,
      "num_children": 0,
      "score": 0.739549994468689,
      "strategy": "Adopt a dynamic strategy where the row player adjusts their move based on the observed history of th"
    },
    "root_2": {
      "depth": 1,
      "num_children": 0,
      "score": 0.6809799671173096,
      "strategy": "Use a mixed strategy where the row player chooses 'Coordination' with probability 0.6 and 'Conflict'"
    },
    "root_0_0": {
      "depth": 2,
      "num_children": 1,
      "score": 1.2194498777389526,
      "strategy": "Implement a reinforcement learning-based strategy where the row player adjusts its action based on t"
    },
    "root_0_0_0": {
      "depth": 3,
      "num_children": 1,
      "score": 1.1552199125289917,
      "strategy": "Implement a genetic algorithm-based evolutionary strategy where the row player maintains a populatio"
    },
    "root_0_0_0_0": {
      "depth": 4,
      "num_children": 1,
      "score": 1.0897399187088013,
      "strategy": "Implement a reinforcement learning-based policy gradient strategy where the row player learns an act"
    },
    "root_0_0_0_0_0": {
      "depth": 5,
      "num_children": 1,
      "score": 1.1602400541305542,
      "strategy": "Implement a kernel-based Gaussian process regression model that predicts opponent behavior as a func"
    },
    "root_0_0_0_0_0_0": {
      "depth": 6,
      "num_children": 1,
      "score": 1.2703601121902466,
      "strategy": "Implement a time-series autoregressive model (ARIMA) with exogenous variables (exog) to forecast opp"
    },
    "root_0_0_0_0_0_0_0": {
      "depth": 7,
      "num_children": 1,
      "score": 1.0908399820327759,
      "strategy": "Replace the ARIMA model with a non-parametric kernel density estimation (KDE) approach to estimate t"
    },
    "root_0_0_0_0_0_0_0_0": {
      "depth": 8,
      "num_children": 0,
      "score": 1.097540020942688,
      "strategy": "Use a recurrent neural network (RNN) with a GRU layer to model the opponent's action sequence as a t"
    }
  },
  "nodes": {
    "root": {
      "node_id": "root",
      "parent_id": null,
      "depth": 0,
      "strategy": "Baseline (no model execution)",
      "score": 1.0226699113845825,
      "actions_count": 0,
      "children": [
        "root_0",
        "root_1",
        "root_2"
      ],
      "error": null
    },
    "root_0": {
      "node_id": "root_0",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Use a mixed strategy where the row player randomizes between coordinating with the column player and",
      "score": 0.866890013217926,
      "actions_count": 2,
      "children": [
        "root_0_0"
      ],
      "error": null
    },
    "root_1": {
      "node_id": "root_1",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a dynamic strategy where the row player adjusts their move based on the observed history of th",
      "score": 0.739549994468689,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_2": {
      "node_id": "root_2",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Use a mixed strategy where the row player chooses 'Coordination' with probability 0.6 and 'Conflict'",
      "score": 0.6809799671173096,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_0_0": {
      "node_id": "root_0_0",
      "parent_id": "root_0",
      "depth": 2,
      "strategy": "Implement a reinforcement learning-based strategy where the row player adjusts its action based on t",
      "score": 1.2194498777389526,
      "actions_count": 2,
      "children": [
        "root_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0": {
      "node_id": "root_0_0_0",
      "parent_id": "root_0_0",
      "depth": 3,
      "strategy": "Implement a genetic algorithm-based evolutionary strategy where the row player maintains a populatio",
      "score": 1.1552199125289917,
      "actions_count": 2,
      "children": [
        "root_0_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0_0": {
      "node_id": "root_0_0_0_0",
      "parent_id": "root_0_0_0",
      "depth": 4,
      "strategy": "Implement a reinforcement learning-based policy gradient strategy where the row player learns an act",
      "score": 1.0897399187088013,
      "actions_count": 2,
      "children": [
        "root_0_0_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0_0_0": {
      "node_id": "root_0_0_0_0_0",
      "parent_id": "root_0_0_0_0",
      "depth": 5,
      "strategy": "Implement a kernel-based Gaussian process regression model that predicts opponent behavior as a func",
      "score": 1.1602400541305542,
      "actions_count": 2,
      "children": [
        "root_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0_0_0_0": {
      "node_id": "root_0_0_0_0_0_0",
      "parent_id": "root_0_0_0_0_0",
      "depth": 6,
      "strategy": "Implement a time-series autoregressive model (ARIMA) with exogenous variables (exog) to forecast opp",
      "score": 1.2703601121902466,
      "actions_count": 2,
      "children": [
        "root_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0_0_0_0_0": {
      "node_id": "root_0_0_0_0_0_0_0",
      "parent_id": "root_0_0_0_0_0_0",
      "depth": 7,
      "strategy": "Replace the ARIMA model with a non-parametric kernel density estimation (KDE) approach to estimate t",
      "score": 1.0908399820327759,
      "actions_count": 2,
      "children": [
        "root_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0_0_0_0_0_0": {
      "node_id": "root_0_0_0_0_0_0_0_0",
      "parent_id": "root_0_0_0_0_0_0_0",
      "depth": 8,
      "strategy": "Use a recurrent neural network (RNN) with a GRU layer to model the opponent's action sequence as a t",
      "score": 1.097540020942688,
      "actions_count": 2,
      "children": [],
      "error": null
    }
  }
}