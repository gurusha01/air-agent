{
  "task": "Battle of Sexes",
  "primary_metric": "Score",
  "higher_is_better": true,
  "selection_strategy": "open-ended",
  "best_node_id": "root_0",
  "best_score": 1.1605899333953857,
  "baseline_score": 1.0226699113845825,
  "improvement": 0.13792002201080322,
  "total_nodes": 31,
  "elapsed_seconds": 820.7,
  "tree_shape": {
    "root": {
      "depth": 0,
      "num_children": 3,
      "score": 1.0226699113845825,
      "strategy": "Baseline (no model execution)"
    },
    "root_0": {
      "depth": 1,
      "num_children": 1,
      "score": 1.1605899333953857,
      "strategy": "Adopt a randomized alternating strategy where the row player chooses Action A with probability 0.6 a"
    },
    "root_1": {
      "depth": 1,
      "num_children": 1,
      "score": 1.0724900960922241,
      "strategy": "Adopt a mixed strategy where the row player randomizes between playing 'Left' and 'Right' with a pro"
    },
    "root_2": {
      "depth": 1,
      "num_children": 0,
      "score": 0.8619299530982971,
      "strategy": "Adopt a mixed strategy where the row player randomly chooses to coordinate with the column player (b"
    },
    "root_0_0": {
      "depth": 2,
      "num_children": 0,
      "score": 0.7249100804328918,
      "strategy": "Adopt a time-dependent probabilistic strategy where the row player adjusts the probability of choosi"
    },
    "root_1_0": {
      "depth": 2,
      "num_children": 1,
      "score": 1.0783900022506714,
      "strategy": "Adopt a time-series autoregressive strategy where the row player's choice in each round is determine"
    },
    "root_1_0_0": {
      "depth": 3,
      "num_children": 1,
      "score": 1.0739800930023193,
      "strategy": "Adopt a Bayesian belief updating strategy where the row player starts with a uniform prior over the "
    },
    "root_1_0_0_0": {
      "depth": 4,
      "num_children": 1,
      "score": 1.0775599479675293,
      "strategy": "Adopt a reinforcement learning with exploration-exploitation (\u03b5-greedy) strategy where the row playe"
    },
    "root_1_0_0_0_0": {
      "depth": 5,
      "num_children": 1,
      "score": 1.0813499689102173,
      "strategy": "Adopt a contextual bandit strategy with a dynamic feature vector that encodes both the opponent's pr"
    },
    "root_1_0_0_0_0_0": {
      "depth": 6,
      "num_children": 1,
      "score": 1.0734199285507202,
      "strategy": "Adopt a recurrent neural network (RNN) with a long short-term memory (LSTM) architecture to model th"
    },
    "root_1_0_0_0_0_0_0": {
      "depth": 7,
      "num_children": 1,
      "score": 1.0752099752426147,
      "strategy": "Replace the LSTM-based RNN with a Transformer-based architecture that models the action history usin"
    },
    "root_1_0_0_0_0_0_0_0": {
      "depth": 8,
      "num_children": 1,
      "score": 1.074290156364441,
      "strategy": "Introduce a graph neural network (GNN) that models the interaction history as a dynamic graph, where"
    },
    "root_1_0_0_0_0_0_0_0_0": {
      "depth": 9,
      "num_children": 1,
      "score": 1.0772700309753418,
      "strategy": "Replace the GNN with a Transformer-based sequence model that encodes action histories as time-series"
    },
    "root_1_0_0_0_0_0_0_0_0_0": {
      "depth": 10,
      "num_children": 1,
      "score": 1.076159954071045,
      "strategy": "Replace the Transformer with a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) uni"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0": {
      "depth": 11,
      "num_children": 1,
      "score": 1.079390048980713,
      "strategy": "Introduce a Hidden Markov Model (HMM) with observable action sequences and latent state transitions,"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 12,
      "num_children": 1,
      "score": 1.0734601020812988,
      "strategy": "Replace the Hidden Markov Model with a Recurrent Neural Network (RNN) that processes sequential oppo"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 13,
      "num_children": 1,
      "score": 1.076449990272522,
      "strategy": "Replace the RNN with a Transformer-based architecture that processes opponent actions as a sequence "
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 14,
      "num_children": 1,
      "score": 1.0712300539016724,
      "strategy": "Introduce a memory-augmented neural network using a Neural Turing Machine (NTM) architecture, where "
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 15,
      "num_children": 1,
      "score": 1.0771600008010864,
      "strategy": "Introduce a reinforcement learning with curiosity-driven exploration using a Deep Q-Network (DQN) wi"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 16,
      "num_children": 1,
      "score": 1.0762500762939453,
      "strategy": "Implement a meta-learned strategy using a transformer-based policy network where the row player mode"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 17,
      "num_children": 1,
      "score": 1.0773299932479858,
      "strategy": "Use a Bayesian nonparametric hidden Markov model (HMM) to infer the opponent's state transitions ove"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 18,
      "num_children": 1,
      "score": 1.0789999961853027,
      "strategy": "Adopt a reinforcement learning with experience replay (PPO-style) framework where the row player lea"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 19,
      "num_children": 1,
      "score": 1.069809913635254,
      "strategy": "Use a Bayesian nonparametric mixture model (specifically, a Dirichlet process mixture of experts) to"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 20,
      "num_children": 1,
      "score": 1.0791499614715576,
      "strategy": "Apply a recurrent neural network (RNN) with a temporal attention mechanism to model opponent behavio"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 21,
      "num_children": 1,
      "score": 1.0758700370788574,
      "strategy": "Implement a transformer-based sequence model with self-attention over both row and column player act"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 22,
      "num_children": 1,
      "score": 1.073349952697754,
      "strategy": "Adopt a reinforcement learning with intrinsic motivation framework where the row player learns to ma"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 23,
      "num_children": 1,
      "score": 1.0748602151870728,
      "strategy": "Implement a temporal difference learning framework with adversarial reward shaping, where the row pl"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 24,
      "num_children": 1,
      "score": 1.082200050354004,
      "strategy": "Implement a reinforcement learning framework using a variant of Policy Gradient with entropy regular"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 25,
      "num_children": 1,
      "score": 1.0774500370025635,
      "strategy": "Introduce a Bayesian belief updating mechanism where the row player maintains a Dirichlet prior over"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 26,
      "num_children": 1,
      "score": 1.0820599794387817,
      "strategy": "Implement a reinforcement learning with exploration bonus (UCB-style) where the row player treats ea"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 27,
      "num_children": 0,
      "score": 1.0754600763320923,
      "strategy": "Adopt a mixed-strategy equilibrium prediction model where the row player estimates the opponent's pr"
    }
  },
  "nodes": {
    "root": {
      "node_id": "root",
      "parent_id": null,
      "depth": 0,
      "strategy": "Baseline (no model execution)",
      "score": 1.0226699113845825,
      "actions_count": 0,
      "children": [
        "root_0",
        "root_1",
        "root_2"
      ],
      "error": null
    },
    "root_0": {
      "node_id": "root_0",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a randomized alternating strategy where the row player chooses Action A with probability 0.6 a",
      "score": 1.1605899333953857,
      "actions_count": 2,
      "children": [
        "root_0_0"
      ],
      "error": null
    },
    "root_1": {
      "node_id": "root_1",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a mixed strategy where the row player randomizes between playing 'Left' and 'Right' with a pro",
      "score": 1.0724900960922241,
      "actions_count": 2,
      "children": [
        "root_1_0"
      ],
      "error": null
    },
    "root_2": {
      "node_id": "root_2",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a mixed strategy where the row player randomly chooses to coordinate with the column player (b",
      "score": 0.8619299530982971,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_0_0": {
      "node_id": "root_0_0",
      "parent_id": "root_0",
      "depth": 2,
      "strategy": "Adopt a time-dependent probabilistic strategy where the row player adjusts the probability of choosi",
      "score": 0.7249100804328918,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_1_0": {
      "node_id": "root_1_0",
      "parent_id": "root_1",
      "depth": 2,
      "strategy": "Adopt a time-series autoregressive strategy where the row player's choice in each round is determine",
      "score": 1.0783900022506714,
      "actions_count": 2,
      "children": [
        "root_1_0_0"
      ],
      "error": null
    },
    "root_1_0_0": {
      "node_id": "root_1_0_0",
      "parent_id": "root_1_0",
      "depth": 3,
      "strategy": "Adopt a Bayesian belief updating strategy where the row player starts with a uniform prior over the ",
      "score": 1.0739800930023193,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0": {
      "node_id": "root_1_0_0_0",
      "parent_id": "root_1_0_0",
      "depth": 4,
      "strategy": "Adopt a reinforcement learning with exploration-exploitation (\u03b5-greedy) strategy where the row playe",
      "score": 1.0775599479675293,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0": {
      "node_id": "root_1_0_0_0_0",
      "parent_id": "root_1_0_0_0",
      "depth": 5,
      "strategy": "Adopt a contextual bandit strategy with a dynamic feature vector that encodes both the opponent's pr",
      "score": 1.0813499689102173,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0",
      "depth": 6,
      "strategy": "Adopt a recurrent neural network (RNN) with a long short-term memory (LSTM) architecture to model th",
      "score": 1.0734199285507202,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0",
      "depth": 7,
      "strategy": "Replace the LSTM-based RNN with a Transformer-based architecture that models the action history usin",
      "score": 1.0752099752426147,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0",
      "depth": 8,
      "strategy": "Introduce a graph neural network (GNN) that models the interaction history as a dynamic graph, where",
      "score": 1.074290156364441,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0",
      "depth": 9,
      "strategy": "Replace the GNN with a Transformer-based sequence model that encodes action histories as time-series",
      "score": 1.0772700309753418,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0",
      "depth": 10,
      "strategy": "Replace the Transformer with a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) uni",
      "score": 1.076159954071045,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0",
      "depth": 11,
      "strategy": "Introduce a Hidden Markov Model (HMM) with observable action sequences and latent state transitions,",
      "score": 1.079390048980713,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0",
      "depth": 12,
      "strategy": "Replace the Hidden Markov Model with a Recurrent Neural Network (RNN) that processes sequential oppo",
      "score": 1.0734601020812988,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 13,
      "strategy": "Replace the RNN with a Transformer-based architecture that processes opponent actions as a sequence ",
      "score": 1.076449990272522,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 14,
      "strategy": "Introduce a memory-augmented neural network using a Neural Turing Machine (NTM) architecture, where ",
      "score": 1.0712300539016724,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 15,
      "strategy": "Introduce a reinforcement learning with curiosity-driven exploration using a Deep Q-Network (DQN) wi",
      "score": 1.0771600008010864,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 16,
      "strategy": "Implement a meta-learned strategy using a transformer-based policy network where the row player mode",
      "score": 1.0762500762939453,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 17,
      "strategy": "Use a Bayesian nonparametric hidden Markov model (HMM) to infer the opponent's state transitions ove",
      "score": 1.0773299932479858,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 18,
      "strategy": "Adopt a reinforcement learning with experience replay (PPO-style) framework where the row player lea",
      "score": 1.0789999961853027,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 19,
      "strategy": "Use a Bayesian nonparametric mixture model (specifically, a Dirichlet process mixture of experts) to",
      "score": 1.069809913635254,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 20,
      "strategy": "Apply a recurrent neural network (RNN) with a temporal attention mechanism to model opponent behavio",
      "score": 1.0791499614715576,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 21,
      "strategy": "Implement a transformer-based sequence model with self-attention over both row and column player act",
      "score": 1.0758700370788574,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 22,
      "strategy": "Adopt a reinforcement learning with intrinsic motivation framework where the row player learns to ma",
      "score": 1.073349952697754,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 23,
      "strategy": "Implement a temporal difference learning framework with adversarial reward shaping, where the row pl",
      "score": 1.0748602151870728,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 24,
      "strategy": "Implement a reinforcement learning framework using a variant of Policy Gradient with entropy regular",
      "score": 1.082200050354004,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 25,
      "strategy": "Introduce a Bayesian belief updating mechanism where the row player maintains a Dirichlet prior over",
      "score": 1.0774500370025635,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 26,
      "strategy": "Implement a reinforcement learning with exploration bonus (UCB-style) where the row player treats ea",
      "score": 1.0820599794387817,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 27,
      "strategy": "Adopt a mixed-strategy equilibrium prediction model where the row player estimates the opponent's pr",
      "score": 1.0754600763320923,
      "actions_count": 2,
      "children": [],
      "error": null
    }
  }
}