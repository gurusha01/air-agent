{
  "task": "Battle of Sexes",
  "primary_metric": "Score",
  "higher_is_better": true,
  "selection_strategy": "ucb",
  "best_node_id": "root_2_0_0_0",
  "best_score": 1.1053099632263184,
  "baseline_score": 1.0226699113845825,
  "improvement": 0.08264005184173584,
  "total_nodes": 31,
  "elapsed_seconds": 555.5,
  "tree_shape": {
    "root": {
      "depth": 0,
      "num_children": 3,
      "score": 1.0226699113845825,
      "strategy": "Baseline (no model execution)"
    },
    "root_0": {
      "depth": 1,
      "num_children": 1,
      "score": 0.8005099892616272,
      "strategy": "Use a mixed strategy where the row player randomizes between coordinating with the column player and"
    },
    "root_1": {
      "depth": 1,
      "num_children": 1,
      "score": 0.7572400569915771,
      "strategy": "Adopt a dynamic frequency-based strategy where the row player adjusts their choice (cooperate or def"
    },
    "root_2": {
      "depth": 1,
      "num_children": 1,
      "score": 0.9740899205207825,
      "strategy": "Adopt a randomized alternating strategy where the row player chooses \"Movie\" with probability 0.6 an"
    },
    "root_2_0": {
      "depth": 2,
      "num_children": 1,
      "score": 0.8059999942779541,
      "strategy": "Adopt a time-dependent phase-based strategy where the row player cycles through three distinct proba"
    },
    "root_2_0_0": {
      "depth": 3,
      "num_children": 1,
      "score": 0.9357900619506836,
      "strategy": "Adopt a dynamically adaptive strategy using real-time opponent response tracking: after each round, "
    },
    "root_2_0_0_0": {
      "depth": 4,
      "num_children": 1,
      "score": 1.1053099632263184,
      "strategy": "Apply a time-delayed feedback mechanism where the row player's choice probability is updated based o"
    },
    "root_2_0_0_0_0": {
      "depth": 5,
      "num_children": 1,
      "score": 1.0681999921798706,
      "strategy": "Apply a phase-shifted reinforcement learning strategy where the row player's choice probability is a"
    },
    "root_2_0_0_0_0_0": {
      "depth": 6,
      "num_children": 1,
      "score": 0.9552200436592102,
      "strategy": "Implement a Bayesian belief updating strategy where the row player maintains a posterior distributio"
    },
    "root_2_0_0_0_0_0_0": {
      "depth": 7,
      "num_children": 1,
      "score": 0.9126099348068237,
      "strategy": "Implement a reinforcement learning-based strategy where the row player learns a policy through trial"
    },
    "root_2_0_0_0_0_0_0_0": {
      "depth": 8,
      "num_children": 0,
      "score": 0.3219600021839142,
      "strategy": "Apply a Markov Chain-based transition modeling strategy where the row player predicts the opponent's"
    },
    "root_0_0": {
      "depth": 2,
      "num_children": 1,
      "score": 0.8011999726295471,
      "strategy": "The row player employs a time-delayed feedback strategy where they analyze the column player's actio"
    },
    "root_0_0_0": {
      "depth": 3,
      "num_children": 1,
      "score": 0.8007799983024597,
      "strategy": "The row player implements a quantum-inspired state mixing strategy, where they maintain a probabilis"
    },
    "root_0_0_0_0": {
      "depth": 4,
      "num_children": 0,
      "score": 0.6434699892997742,
      "strategy": "The row player adopts a time-delayed reinforcement learning strategy where they track the column pla"
    },
    "root_1_0": {
      "depth": 2,
      "num_children": 1,
      "score": 0.7983099818229675,
      "strategy": "Implement a reinforcement learning-based strategy where the row player updates their action probabil"
    },
    "root_1_0_0": {
      "depth": 3,
      "num_children": 1,
      "score": 0.8305099606513977,
      "strategy": "Implement a memory-based strategy using a sliding window of the last 3 rounds to compute a moving av"
    },
    "root_1_0_0_0": {
      "depth": 4,
      "num_children": 1,
      "score": 0.7355700135231018,
      "strategy": "Implement a strategy based on the autocorrelation of the column player's actions over a 5-round wind"
    },
    "root_1_0_0_0_0": {
      "depth": 5,
      "num_children": 1,
      "score": 0.7650499939918518,
      "strategy": "Implement a strategy using hidden Markov models (HMMs) to infer the column player's underlying state"
    },
    "root_1_0_0_0_0_0": {
      "depth": 6,
      "num_children": 1,
      "score": 0.8021000623703003,
      "strategy": "Implement a reinforcement learning-based strategy where the row player uses a Q-learning agent to le"
    },
    "root_1_0_0_0_0_0_0": {
      "depth": 7,
      "num_children": 1,
      "score": 0.8031299710273743,
      "strategy": "Implement a contextual bandit strategy where the row player uses a linear model with features derive"
    },
    "root_1_0_0_0_0_0_0_0": {
      "depth": 8,
      "num_children": 1,
      "score": 0.7117899656295776,
      "strategy": "Implement a recurrent neural network (RNN) with an LSTM layer to model the temporal dependencies in "
    },
    "root_1_0_0_0_0_0_0_0_0": {
      "depth": 9,
      "num_children": 1,
      "score": 0.6866000294685364,
      "strategy": "Implement a Transformer-based model with self-attention over action sequences, where each token repr"
    },
    "root_1_0_0_0_0_0_0_0_0_0": {
      "depth": 10,
      "num_children": 1,
      "score": 0.684469997882843,
      "strategy": "Replace the Transformer with a convolutional neural network (CNN) that processes the move history as"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0": {
      "depth": 11,
      "num_children": 1,
      "score": 0.6817200183868408,
      "strategy": "Replace the CNN with a recurrent neural network (RNN) using a bidirectional Long Short-Term Memory ("
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 12,
      "num_children": 1,
      "score": 0.695169985294342,
      "strategy": "Introduce a Transformer-based attention mechanism that models the Battle of Sexes game as a sequenti"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 13,
      "num_children": 1,
      "score": 0.7109799981117249,
      "strategy": "Introduce a Gaussian Process-based Bayesian inference framework to model the row player's payoff as "
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 14,
      "num_children": 1,
      "score": 0.7225300073623657,
      "strategy": "Introduce a random forest-based ensemble model that learns payoff predictions from discretized oppon"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 15,
      "num_children": 1,
      "score": 0.7405099868774414,
      "strategy": "Introduce a latent Dirichlet allocation (LDA)-based topic modeling approach to extract hidden behavi"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 16,
      "num_children": 1,
      "score": 0.7353799939155579,
      "strategy": "Apply a recurrent neural network (RNN) with a long short-term memory (LSTM) layer to model opponent "
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 17,
      "num_children": 1,
      "score": 0.7368900179862976,
      "strategy": "Apply a random forest classifier trained on lagged opponent action sequences, where each feature rep"
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 18,
      "num_children": 0,
      "score": 0.676770031452179,
      "strategy": "Switch to a support vector machine (SVM) classifier trained on a sliding window of opponent actions,"
    }
  },
  "nodes": {
    "root": {
      "node_id": "root",
      "parent_id": null,
      "depth": 0,
      "strategy": "Baseline (no model execution)",
      "score": 1.0226699113845825,
      "actions_count": 0,
      "children": [
        "root_0",
        "root_1",
        "root_2"
      ],
      "error": null
    },
    "root_0": {
      "node_id": "root_0",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Use a mixed strategy where the row player randomizes between coordinating with the column player and",
      "score": 0.8005099892616272,
      "actions_count": 2,
      "children": [
        "root_0_0"
      ],
      "error": null
    },
    "root_1": {
      "node_id": "root_1",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a dynamic frequency-based strategy where the row player adjusts their choice (cooperate or def",
      "score": 0.7572400569915771,
      "actions_count": 2,
      "children": [
        "root_1_0"
      ],
      "error": null
    },
    "root_2": {
      "node_id": "root_2",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a randomized alternating strategy where the row player chooses \"Movie\" with probability 0.6 an",
      "score": 0.9740899205207825,
      "actions_count": 2,
      "children": [
        "root_2_0"
      ],
      "error": null
    },
    "root_2_0": {
      "node_id": "root_2_0",
      "parent_id": "root_2",
      "depth": 2,
      "strategy": "Adopt a time-dependent phase-based strategy where the row player cycles through three distinct proba",
      "score": 0.8059999942779541,
      "actions_count": 2,
      "children": [
        "root_2_0_0"
      ],
      "error": null
    },
    "root_2_0_0": {
      "node_id": "root_2_0_0",
      "parent_id": "root_2_0",
      "depth": 3,
      "strategy": "Adopt a dynamically adaptive strategy using real-time opponent response tracking: after each round, ",
      "score": 0.9357900619506836,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0": {
      "node_id": "root_2_0_0_0",
      "parent_id": "root_2_0_0",
      "depth": 4,
      "strategy": "Apply a time-delayed feedback mechanism where the row player's choice probability is updated based o",
      "score": 1.1053099632263184,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0_0": {
      "node_id": "root_2_0_0_0_0",
      "parent_id": "root_2_0_0_0",
      "depth": 5,
      "strategy": "Apply a phase-shifted reinforcement learning strategy where the row player's choice probability is a",
      "score": 1.0681999921798706,
      "actions_count": 4,
      "children": [
        "root_2_0_0_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0_0_0": {
      "node_id": "root_2_0_0_0_0_0",
      "parent_id": "root_2_0_0_0_0",
      "depth": 6,
      "strategy": "Implement a Bayesian belief updating strategy where the row player maintains a posterior distributio",
      "score": 0.9552200436592102,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0_0_0_0": {
      "node_id": "root_2_0_0_0_0_0_0",
      "parent_id": "root_2_0_0_0_0_0",
      "depth": 7,
      "strategy": "Implement a reinforcement learning-based strategy where the row player learns a policy through trial",
      "score": 0.9126099348068237,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0_0_0_0_0": {
      "node_id": "root_2_0_0_0_0_0_0_0",
      "parent_id": "root_2_0_0_0_0_0_0",
      "depth": 8,
      "strategy": "Apply a Markov Chain-based transition modeling strategy where the row player predicts the opponent's",
      "score": 0.3219600021839142,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_0_0": {
      "node_id": "root_0_0",
      "parent_id": "root_0",
      "depth": 2,
      "strategy": "The row player employs a time-delayed feedback strategy where they analyze the column player's actio",
      "score": 0.8011999726295471,
      "actions_count": 2,
      "children": [
        "root_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0": {
      "node_id": "root_0_0_0",
      "parent_id": "root_0_0",
      "depth": 3,
      "strategy": "The row player implements a quantum-inspired state mixing strategy, where they maintain a probabilis",
      "score": 0.8007799983024597,
      "actions_count": 2,
      "children": [
        "root_0_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0_0": {
      "node_id": "root_0_0_0_0",
      "parent_id": "root_0_0_0",
      "depth": 4,
      "strategy": "The row player adopts a time-delayed reinforcement learning strategy where they track the column pla",
      "score": 0.6434699892997742,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_1_0": {
      "node_id": "root_1_0",
      "parent_id": "root_1",
      "depth": 2,
      "strategy": "Implement a reinforcement learning-based strategy where the row player updates their action probabil",
      "score": 0.7983099818229675,
      "actions_count": 2,
      "children": [
        "root_1_0_0"
      ],
      "error": null
    },
    "root_1_0_0": {
      "node_id": "root_1_0_0",
      "parent_id": "root_1_0",
      "depth": 3,
      "strategy": "Implement a memory-based strategy using a sliding window of the last 3 rounds to compute a moving av",
      "score": 0.8305099606513977,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0": {
      "node_id": "root_1_0_0_0",
      "parent_id": "root_1_0_0",
      "depth": 4,
      "strategy": "Implement a strategy based on the autocorrelation of the column player's actions over a 5-round wind",
      "score": 0.7355700135231018,
      "actions_count": 10,
      "children": [
        "root_1_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0": {
      "node_id": "root_1_0_0_0_0",
      "parent_id": "root_1_0_0_0",
      "depth": 5,
      "strategy": "Implement a strategy using hidden Markov models (HMMs) to infer the column player's underlying state",
      "score": 0.7650499939918518,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0",
      "depth": 6,
      "strategy": "Implement a reinforcement learning-based strategy where the row player uses a Q-learning agent to le",
      "score": 0.8021000623703003,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0",
      "depth": 7,
      "strategy": "Implement a contextual bandit strategy where the row player uses a linear model with features derive",
      "score": 0.8031299710273743,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0",
      "depth": 8,
      "strategy": "Implement a recurrent neural network (RNN) with an LSTM layer to model the temporal dependencies in ",
      "score": 0.7117899656295776,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0",
      "depth": 9,
      "strategy": "Implement a Transformer-based model with self-attention over action sequences, where each token repr",
      "score": 0.6866000294685364,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0",
      "depth": 10,
      "strategy": "Replace the Transformer with a convolutional neural network (CNN) that processes the move history as",
      "score": 0.684469997882843,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0",
      "depth": 11,
      "strategy": "Replace the CNN with a recurrent neural network (RNN) using a bidirectional Long Short-Term Memory (",
      "score": 0.6817200183868408,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0",
      "depth": 12,
      "strategy": "Introduce a Transformer-based attention mechanism that models the Battle of Sexes game as a sequenti",
      "score": 0.695169985294342,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 13,
      "strategy": "Introduce a Gaussian Process-based Bayesian inference framework to model the row player's payoff as ",
      "score": 0.7109799981117249,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 14,
      "strategy": "Introduce a random forest-based ensemble model that learns payoff predictions from discretized oppon",
      "score": 0.7225300073623657,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 15,
      "strategy": "Introduce a latent Dirichlet allocation (LDA)-based topic modeling approach to extract hidden behavi",
      "score": 0.7405099868774414,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 16,
      "strategy": "Apply a recurrent neural network (RNN) with a long short-term memory (LSTM) layer to model opponent ",
      "score": 0.7353799939155579,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 17,
      "strategy": "Apply a random forest classifier trained on lagged opponent action sequences, where each feature rep",
      "score": 0.7368900179862976,
      "actions_count": 2,
      "children": [
        "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 18,
      "strategy": "Switch to a support vector machine (SVM) classifier trained on a sliding window of opponent actions,",
      "score": 0.676770031452179,
      "actions_count": 2,
      "children": [],
      "error": null
    }
  }
}