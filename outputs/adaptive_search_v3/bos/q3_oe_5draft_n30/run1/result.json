{
  "task": "Battle of Sexes",
  "primary_metric": "Score",
  "higher_is_better": true,
  "selection_strategy": "open-ended",
  "best_node_id": "root_3_0_0_0_0_0_0_0_0_0",
  "best_score": 1.4414000511169434,
  "baseline_score": 1.0226699113845825,
  "improvement": 0.41873013973236084,
  "total_nodes": 31,
  "elapsed_seconds": 583.4,
  "tree_shape": {
    "root": {
      "depth": 0,
      "num_children": 5,
      "score": 1.0226699113845825,
      "strategy": "Baseline (no model execution)"
    },
    "root_0": {
      "depth": 1,
      "num_children": 1,
      "score": 0.9091299772262573,
      "strategy": "Adopt a time-varying mixed strategy where the row player adjusts their choice (Cooperate or Defect) "
    },
    "root_1": {
      "depth": 1,
      "num_children": 0,
      "score": 0.6698299646377563,
      "strategy": "Adopt a mixed strategy where the row player chooses Action A with probability 0.7 and Action B with "
    },
    "root_2": {
      "depth": 1,
      "num_children": 1,
      "score": 0.8012099862098694,
      "strategy": "Adopt a mixed strategy where the row player randomizes between choosing 'Movie' and 'Sports' with a "
    },
    "root_3": {
      "depth": 1,
      "num_children": 1,
      "score": 0.7250999808311462,
      "strategy": "Use a mixed strategy where the row player randomly selects \"Cooperate\" with probability 0.6 and \"Def"
    },
    "root_4": {
      "depth": 1,
      "num_children": 1,
      "score": 0.7641500234603882,
      "strategy": "Adopt a time-varying mixed strategy where the row player randomizes between coordinating with the co"
    },
    "root_0_0": {
      "depth": 2,
      "num_children": 1,
      "score": 0.902660071849823,
      "strategy": "Implement a reinforcement learning-based strategy where the row player uses a Q-learning model with "
    },
    "root_0_0_0": {
      "depth": 3,
      "num_children": 1,
      "score": 0.8502299785614014,
      "strategy": "Use a recurrent neural network (RNN) with a GRU layer to model the temporal dynamics of the opponent"
    },
    "root_0_0_0_0": {
      "depth": 4,
      "num_children": 1,
      "score": 0.8915099501609802,
      "strategy": "Use a shallow decision tree with handcrafted features derived from the opponent's action history, in"
    },
    "root_0_0_0_0_0": {
      "depth": 5,
      "num_children": 0,
      "score": 0.6693699955940247,
      "strategy": "Apply a sliding-window correlation-based strategy that tracks the pairwise correlation between the o"
    },
    "root_2_0": {
      "depth": 2,
      "num_children": 1,
      "score": 0.731529951095581,
      "strategy": "Implement a time-series adaptive strategy where the row player uses a simple moving average of the o"
    },
    "root_4_0": {
      "depth": 2,
      "num_children": 0,
      "score": 0.5790199637413025,
      "strategy": "Adopt a reinforcement learning-based policy where the row player updates its action probability usin"
    },
    "root_2_0_0": {
      "depth": 3,
      "num_children": 1,
      "score": 1.1631100177764893,
      "strategy": "Implement a sinusoidal phase-shift strategy where the row player's choice is determined by the phase"
    },
    "root_2_0_0_0": {
      "depth": 4,
      "num_children": 1,
      "score": 0.7279700040817261,
      "strategy": "Implement a geometric decay-based strategy where the row player's choice in round t is determined by"
    },
    "root_2_0_0_0_0": {
      "depth": 5,
      "num_children": 1,
      "score": 0.8705499768257141,
      "strategy": "The row player adopts a random walk with drift strategy: in each round, the choice is determined by "
    },
    "root_2_0_0_0_0_0": {
      "depth": 6,
      "num_children": 1,
      "score": 0.8565900325775146,
      "strategy": "The row player employs a sinusoidal drift strategy where the probability of choosing 'Movie' follows"
    },
    "root_2_0_0_0_0_0_0": {
      "depth": 7,
      "num_children": 1,
      "score": 0.9980199933052063,
      "strategy": "Introduce a phase-shifted cosine drift with adaptive amplitude modulation. The probability of choosi"
    },
    "root_2_0_0_0_0_0_0_0": {
      "depth": 8,
      "num_children": 0,
      "score": 0.6109200119972229,
      "strategy": "Implement a piecewise-linear drift strategy where the probability of choosing 'Movie' increases in a"
    },
    "root_3_0": {
      "depth": 2,
      "num_children": 1,
      "score": 0.7405700087547302,
      "strategy": "Adopt a dynamic adaptive strategy where the row player adjusts the probability of cooperating based "
    },
    "root_3_0_0": {
      "depth": 3,
      "num_children": 1,
      "score": 0.7472200393676758,
      "strategy": "Implement a reinforcement learning-based strategy where the row player updates its cooperation proba"
    },
    "root_3_0_0_0": {
      "depth": 4,
      "num_children": 1,
      "score": 0.8833000063896179,
      "strategy": "Implement a recurrent neural network (RNN) with a GRU layer to model the sequence of column player a"
    },
    "root_3_0_0_0_0": {
      "depth": 5,
      "num_children": 1,
      "score": 0.9921700358390808,
      "strategy": "Implement a convolutional neural network (CNN) with 1D convolutions to extract local temporal patter"
    },
    "root_3_0_0_0_0_0": {
      "depth": 6,
      "num_children": 1,
      "score": 0.9907000064849854,
      "strategy": "Implement a transformer-based model with self-attention over the sequence of column player actions, "
    },
    "root_3_0_0_0_0_0_0": {
      "depth": 7,
      "num_children": 1,
      "score": 0.9930700063705444,
      "strategy": "Apply a recurrent neural network (RNN) with long short-term memory (LSTM) units to model the sequenc"
    },
    "root_3_0_0_0_0_0_0_0": {
      "depth": 8,
      "num_children": 1,
      "score": 1.4412000179290771,
      "strategy": "Introduce a dynamic threshold-based strategy that monitors the average payoff of the column player o"
    },
    "root_3_0_0_0_0_0_0_0_0": {
      "depth": 9,
      "num_children": 1,
      "score": 1.4409198760986328,
      "strategy": "Implement a payoff asymmetry detection strategy that tracks the difference in cumulative payoffs bet"
    },
    "root_3_0_0_0_0_0_0_0_0_0": {
      "depth": 10,
      "num_children": 1,
      "score": 1.4414000511169434,
      "strategy": "Implement a dynamic payoff correlation tracking strategy that computes the rolling correlation coeff"
    },
    "root_3_0_0_0_0_0_0_0_0_0_0": {
      "depth": 11,
      "num_children": 1,
      "score": 0.9680400490760803,
      "strategy": "Implement a reinforcement learning-based policy that uses a Q-learning agent to estimate the value o"
    },
    "root_3_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 12,
      "num_children": 1,
      "score": 0.8101500272750854,
      "strategy": "Implement a dynamic context-aware policy using a hidden Markov model (HMM) that models the column pl"
    },
    "root_3_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 13,
      "num_children": 1,
      "score": 0.9070600867271423,
      "strategy": "Implement a reinforcement learning policy using a Proximal Policy Optimization (PPO) algorithm with "
    },
    "root_3_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "depth": 14,
      "num_children": 0,
      "score": 1.142930030822754,
      "strategy": "Switch to a Bayesian reinforcement learning framework where the row player maintains a belief over t"
    }
  },
  "nodes": {
    "root": {
      "node_id": "root",
      "parent_id": null,
      "depth": 0,
      "strategy": "Baseline (no model execution)",
      "score": 1.0226699113845825,
      "actions_count": 0,
      "children": [
        "root_0",
        "root_1",
        "root_2",
        "root_3",
        "root_4"
      ],
      "error": null
    },
    "root_0": {
      "node_id": "root_0",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a time-varying mixed strategy where the row player adjusts their choice (Cooperate or Defect) ",
      "score": 0.9091299772262573,
      "actions_count": 2,
      "children": [
        "root_0_0"
      ],
      "error": null
    },
    "root_1": {
      "node_id": "root_1",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a mixed strategy where the row player chooses Action A with probability 0.7 and Action B with ",
      "score": 0.6698299646377563,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_2": {
      "node_id": "root_2",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a mixed strategy where the row player randomizes between choosing 'Movie' and 'Sports' with a ",
      "score": 0.8012099862098694,
      "actions_count": 2,
      "children": [
        "root_2_0"
      ],
      "error": null
    },
    "root_3": {
      "node_id": "root_3",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Use a mixed strategy where the row player randomly selects \"Cooperate\" with probability 0.6 and \"Def",
      "score": 0.7250999808311462,
      "actions_count": 2,
      "children": [
        "root_3_0"
      ],
      "error": null
    },
    "root_4": {
      "node_id": "root_4",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a time-varying mixed strategy where the row player randomizes between coordinating with the co",
      "score": 0.7641500234603882,
      "actions_count": 2,
      "children": [
        "root_4_0"
      ],
      "error": null
    },
    "root_0_0": {
      "node_id": "root_0_0",
      "parent_id": "root_0",
      "depth": 2,
      "strategy": "Implement a reinforcement learning-based strategy where the row player uses a Q-learning model with ",
      "score": 0.902660071849823,
      "actions_count": 2,
      "children": [
        "root_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0": {
      "node_id": "root_0_0_0",
      "parent_id": "root_0_0",
      "depth": 3,
      "strategy": "Use a recurrent neural network (RNN) with a GRU layer to model the temporal dynamics of the opponent",
      "score": 0.8502299785614014,
      "actions_count": 4,
      "children": [
        "root_0_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0_0": {
      "node_id": "root_0_0_0_0",
      "parent_id": "root_0_0_0",
      "depth": 4,
      "strategy": "Use a shallow decision tree with handcrafted features derived from the opponent's action history, in",
      "score": 0.8915099501609802,
      "actions_count": 2,
      "children": [
        "root_0_0_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0_0_0": {
      "node_id": "root_0_0_0_0_0",
      "parent_id": "root_0_0_0_0",
      "depth": 5,
      "strategy": "Apply a sliding-window correlation-based strategy that tracks the pairwise correlation between the o",
      "score": 0.6693699955940247,
      "actions_count": 4,
      "children": [],
      "error": null
    },
    "root_2_0": {
      "node_id": "root_2_0",
      "parent_id": "root_2",
      "depth": 2,
      "strategy": "Implement a time-series adaptive strategy where the row player uses a simple moving average of the o",
      "score": 0.731529951095581,
      "actions_count": 2,
      "children": [
        "root_2_0_0"
      ],
      "error": null
    },
    "root_4_0": {
      "node_id": "root_4_0",
      "parent_id": "root_4",
      "depth": 2,
      "strategy": "Adopt a reinforcement learning-based policy where the row player updates its action probability usin",
      "score": 0.5790199637413025,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_2_0_0": {
      "node_id": "root_2_0_0",
      "parent_id": "root_2_0",
      "depth": 3,
      "strategy": "Implement a sinusoidal phase-shift strategy where the row player's choice is determined by the phase",
      "score": 1.1631100177764893,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0": {
      "node_id": "root_2_0_0_0",
      "parent_id": "root_2_0_0",
      "depth": 4,
      "strategy": "Implement a geometric decay-based strategy where the row player's choice in round t is determined by",
      "score": 0.7279700040817261,
      "actions_count": 6,
      "children": [
        "root_2_0_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0_0": {
      "node_id": "root_2_0_0_0_0",
      "parent_id": "root_2_0_0_0",
      "depth": 5,
      "strategy": "The row player adopts a random walk with drift strategy: in each round, the choice is determined by ",
      "score": 0.8705499768257141,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0_0_0": {
      "node_id": "root_2_0_0_0_0_0",
      "parent_id": "root_2_0_0_0_0",
      "depth": 6,
      "strategy": "The row player employs a sinusoidal drift strategy where the probability of choosing 'Movie' follows",
      "score": 0.8565900325775146,
      "actions_count": 6,
      "children": [
        "root_2_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0_0_0_0": {
      "node_id": "root_2_0_0_0_0_0_0",
      "parent_id": "root_2_0_0_0_0_0",
      "depth": 7,
      "strategy": "Introduce a phase-shifted cosine drift with adaptive amplitude modulation. The probability of choosi",
      "score": 0.9980199933052063,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0_0_0_0_0": {
      "node_id": "root_2_0_0_0_0_0_0_0",
      "parent_id": "root_2_0_0_0_0_0_0",
      "depth": 8,
      "strategy": "Implement a piecewise-linear drift strategy where the probability of choosing 'Movie' increases in a",
      "score": 0.6109200119972229,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_3_0": {
      "node_id": "root_3_0",
      "parent_id": "root_3",
      "depth": 2,
      "strategy": "Adopt a dynamic adaptive strategy where the row player adjusts the probability of cooperating based ",
      "score": 0.7405700087547302,
      "actions_count": 2,
      "children": [
        "root_3_0_0"
      ],
      "error": null
    },
    "root_3_0_0": {
      "node_id": "root_3_0_0",
      "parent_id": "root_3_0",
      "depth": 3,
      "strategy": "Implement a reinforcement learning-based strategy where the row player updates its cooperation proba",
      "score": 0.7472200393676758,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0": {
      "node_id": "root_3_0_0_0",
      "parent_id": "root_3_0_0",
      "depth": 4,
      "strategy": "Implement a recurrent neural network (RNN) with a GRU layer to model the sequence of column player a",
      "score": 0.8833000063896179,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0_0": {
      "node_id": "root_3_0_0_0_0",
      "parent_id": "root_3_0_0_0",
      "depth": 5,
      "strategy": "Implement a convolutional neural network (CNN) with 1D convolutions to extract local temporal patter",
      "score": 0.9921700358390808,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0_0_0": {
      "node_id": "root_3_0_0_0_0_0",
      "parent_id": "root_3_0_0_0_0",
      "depth": 6,
      "strategy": "Implement a transformer-based model with self-attention over the sequence of column player actions, ",
      "score": 0.9907000064849854,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0_0_0_0": {
      "node_id": "root_3_0_0_0_0_0_0",
      "parent_id": "root_3_0_0_0_0_0",
      "depth": 7,
      "strategy": "Apply a recurrent neural network (RNN) with long short-term memory (LSTM) units to model the sequenc",
      "score": 0.9930700063705444,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0_0_0_0_0": {
      "node_id": "root_3_0_0_0_0_0_0_0",
      "parent_id": "root_3_0_0_0_0_0_0",
      "depth": 8,
      "strategy": "Introduce a dynamic threshold-based strategy that monitors the average payoff of the column player o",
      "score": 1.4412000179290771,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0_0_0_0_0_0": {
      "node_id": "root_3_0_0_0_0_0_0_0_0",
      "parent_id": "root_3_0_0_0_0_0_0_0",
      "depth": 9,
      "strategy": "Implement a payoff asymmetry detection strategy that tracks the difference in cumulative payoffs bet",
      "score": 1.4409198760986328,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_3_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_3_0_0_0_0_0_0_0_0",
      "depth": 10,
      "strategy": "Implement a dynamic payoff correlation tracking strategy that computes the rolling correlation coeff",
      "score": 1.4414000511169434,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_3_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_3_0_0_0_0_0_0_0_0_0",
      "depth": 11,
      "strategy": "Implement a reinforcement learning-based policy that uses a Q-learning agent to estimate the value o",
      "score": 0.9680400490760803,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_3_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_3_0_0_0_0_0_0_0_0_0_0",
      "depth": 12,
      "strategy": "Implement a dynamic context-aware policy using a hidden Markov model (HMM) that models the column pl",
      "score": 0.8101500272750854,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_3_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_3_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 13,
      "strategy": "Implement a reinforcement learning policy using a Proximal Policy Optimization (PPO) algorithm with ",
      "score": 0.9070600867271423,
      "actions_count": 2,
      "children": [
        "root_3_0_0_0_0_0_0_0_0_0_0_0_0_0"
      ],
      "error": null
    },
    "root_3_0_0_0_0_0_0_0_0_0_0_0_0_0": {
      "node_id": "root_3_0_0_0_0_0_0_0_0_0_0_0_0_0",
      "parent_id": "root_3_0_0_0_0_0_0_0_0_0_0_0_0",
      "depth": 14,
      "strategy": "Switch to a Bayesian reinforcement learning framework where the row player maintains a belief over t",
      "score": 1.142930030822754,
      "actions_count": 2,
      "children": [],
      "error": null
    }
  }
}