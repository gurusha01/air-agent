{
  "titanic/gpt4o_aira_evo.g": {
    "root": "Baseline: runs the provided starter code without any model execution. -> 0.766",
    "root_0": "RandomForestClassifier with default hyperparameters (100 trees, no max_depth). Uses features [Pclass, Sex, Age, SibSp, Parch, Fare, Embarked_Q, Embarked_S] with median imputation for Age/Fare, mode imputation for Embarked, and one-hot encoding of Embarked with get_dummies(drop_first=True). -> 0.809",
    "root_1": "Soft VotingClassifier ensemble of RandomForestClassifier(100 trees), GradientBoostingClassifier(100 trees), and LogisticRegression(max_iter=200), all with random_state=42. Uses sklearn ColumnTransformer pipeline with median imputation + StandardScaler for numerics [Age, Fare, FamilySize, Pclass, SibSp, Parch] and mode imputation + OneHotEncoder for categoricals [Sex, Embarked]. Adds FamilySize = SibSp + Parch + 1 as new feature. -> 0.892",
    "root_1_0": "Crossover strategy: Soft VotingClassifier of two RandomForests -- RF1(100 trees, defaults) and RF2(200 trees, max_depth=7). Uses LabelEncoder for Sex, one-hot encoding for Embarked via get_dummies, median imputation for Age/Fare, mode for Embarked; drops Cabin/Name/Ticket. Simpler than parent root_1 (lost GBM and LogReg from ensemble, no StandardScaler, no FamilySize feature). -> 0.883",
    "root_2": "RandomForestClassifier with GridSearchCV over n_estimators=[100,200], max_depth=[5,10,None], min_samples_split=[2,5,10] using 5-fold CV and StandardScaler. Engineers FamilySize feature, uses median imputation for Age, mode for Embarked. Required 3 code rewrites to fix align() dropping Survived column; final version uses join='left' alignment. -> 0.916",
    "root_2_0": "Improvement on root_2: RandomForestClassifier (default hyperparams) with LabelEncoder for Sex/Embarked/Title. Extracts Title from Name via regex ' ([A-Za-z]+)\\.', groups rare titles (Lady, Countess, Capt, Col, etc.) into 'Rare', maps Mlle->Miss, Ms->Miss, Mme->Mrs. No GridSearchCV, no StandardScaler, no FamilySize -- but adds Title feature which parent lacked. -> 0.931",
    "root_2_0_0": "Improvement on root_2_0: RandomForest + GradientBoosting pipeline with GridSearchCV. Adds Cabin_assigned binary feature (1 if cabin known, 0 if not) and uses get_dummies for Embarked. Loses the Title feature from parent root_2_0, uses Sex as numeric map. Pipeline has StandardScaler + SimpleImputer for numerics. Major regression -- lost Title feature caused score to plummet below baseline. -> 0.675",
    "root_2_0_1": "Improvement on root_2_0: RandomForestClassifier with GridSearchCV (n_estimators=[100,200,300], max_depth=[5,10,None], max_features=['sqrt','log2'], min_samples_split=[2,5,10]) using ColumnTransformer pipeline. Engineers FamilySize and Title (with rare title grouping + numeric mapping), uses OneHotEncoder for Pclass/Embarked/Title and StandardScaler for Age/Fare/FamilySize. Initially failed due to mixed str/float types in Title column and 'auto' max_features deprecation; required 3 rewrites to fix. -> 0.907",
    "root_2_0_1_0": "Improvement on root_2_0_1: RandomForestClassifier(200 trees, max_depth=7) in StandardScaler pipeline. Extracts Title via name.split(',')[1].split('.')[0], one-hot encodes Sex/Embarked/Title via get_dummies(drop_first=True), uses align(join='left', fill_value=0) for train/test alignment. Required 3 rewrites to fix embarked imputer 2D output and Survived column leaking into test features. -> 0.907",
    "root_2_0_2": "Improvement on root_2_0: Probability-averaged ensemble of RandomForestClassifier(100 trees, max_depth=5) and GradientBoostingClassifier(100 trees, max_depth=5) with manual 50/50 averaging of predict_proba and 0.5 threshold. Uses Sex as numeric map, get_dummies for Embarked, median/mode imputation; drops Cabin/Name/Ticket. No Title feature (unlike parent root_2_0), and uses shallow max_depth=5 for both models. -> 0.890",
    "root_3": "RandomForestClassifier(100 trees, default hyperparams) with simple preprocessing: Sex mapped to 0/1, Embarked mapped to integers (C=0, Q=1, S=2), median imputation for Age/Fare, mode for Embarked. Drops Name/Ticket/Cabin. First attempt trained on PassengerId (bug); second attempt correctly excludes it. No feature engineering. -> 0.847",
    "root_3_0": "Crossover strategy: Soft VotingClassifier of RandomForestClassifier(100 trees) and GradientBoostingClassifier(100 trees), both with default hyperparameters. Uses get_dummies(drop_first=True) for Sex/Embarked, median imputation for Age/Fare, mode for Embarked. No feature engineering beyond basic encoding. Combines parent root_3's approach with ensemble idea. -> 0.885",
    "root_4": "Soft VotingClassifier ensemble of RandomForestClassifier(100 trees), LogisticRegression(max_iter=1000), and SVC(probability=True). All features StandardScaled after preprocessing. Uses LabelEncoder for Sex, get_dummies for Embarked, SimpleImputer(median) for Age/Fare, mode for Embarked. Drops PassengerId/Name/Ticket/Cabin. Trains on full training set (no validation split). -> 0.919"
  },
  "houseprice/gpt4o_aira_evo.g": {
    "root": "Baseline: runs the provided starter code without any model execution. -> 0.880",
    "root_0": "RandomForestRegressor(100 trees, random_state=0) using sklearn ColumnTransformer pipeline with SimpleImputer(mean) for numerics and SimpleImputer(most_frequent) + OneHotEncoder for categoricals. Auto-detects numeric/object columns, no feature engineering, 80/20 train/test split. -> 0.879",
    "root_0_0": "Improvement on root_0: RandomForestRegressor(200 trees, random_state=42) with median imputation + StandardScaler for numerics and mode imputation + OneHotEncoder for categoricals via ColumnTransformer pipeline. Increased n_estimators from 100 to 200 and switched numeric imputation from mean to median vs. parent root_0. Score slightly decreased. -> 0.877",
    "root_1": "VotingRegressor ensemble of GradientBoostingRegressor, RandomForestRegressor, and ElasticNet (all default params) with GridSearchCV. Preprocesses with fillna(median) for numerics and fillna('None') for categoricals, then pd.get_dummies for one-hot encoding. Initial attempt failed due to align() dropping SalePrice; fixed by separating target before alignment. ElasticNet showed convergence warnings. -> 0.891",
    "root_1_0": "Improvement on root_1: StackingRegressor with GradientBoostingRegressor and RandomForestRegressor as base estimators and Ridge as final estimator. Applies log1p transform to SalePrice target. Uses ColumnTransformer with median imputation + StandardScaler for numerics and mode imputation + OneHotEncoder for categoricals. Fits preprocessor jointly on train, transforms test separately to avoid data leakage. First attempt failed due to different feature counts between train/test preprocessors; fixed by shared fit_transform/transform. -> 0.909",
    "root_1_0_0": "Improvement on root_1_0: StackingRegressor with RandomForestRegressor(100 trees) and GradientBoostingRegressor(100 trees) as base, RidgeCV as meta-learner. Drops log1p target transform (uses raw SalePrice unlike parent). Uses make_pipeline with SimpleImputer(median) + StandardScaler for numerics, then pd.get_dummies for categoricals with align(join='left', fill_value=0). Trains on full dataset (no validation split). Loss of log transform hurt performance vs. parent. -> 0.899",
    "root_1_0_0_0": "Crossover strategy: Weighted average ensemble (0.5/0.5) of GradientBoostingRegressor(300 trees, lr=0.1) and RandomForestRegressor(200 trees). Drops high-null columns [Alley, PoolQC, Fence, MiscFeature], fills LotFrontage with mean, GarageYrBlt with median, MasVnrArea with 0, categorical NaNs with mode. Uses pd.get_dummies with drop_first=True. Struggled with SalePrice leaking into test via align(); required 4 rewrites. -> 0.890",
    "root_2": "GradientBoostingRegressor with GridSearchCV over n_estimators=[100,200], learning_rate=[0.05,0.1], max_depth=[3,4]. Applies log1p transform to SalePrice. Uses ColumnTransformer with median imputation + StandardScaler for numerics and mode imputation + OneHotEncoder for categoricals. Predictions inverse-transformed with expm1. -> 0.887",
    "root_2_0": "Improvement on root_2: Switches from GradientBoostingRegressor to RandomForestRegressor(200 trees, max_depth=25, min_samples_split=5). Drops GridSearchCV and log transform of target from parent. Uses ColumnTransformer pipeline with median imputation + StandardScaler for numerics and constant 'missing' imputation + OneHotEncoder for categoricals. The switch from GBR to RF with no log transform caused slight regression. -> 0.874",
    "root_3": "RandomForestRegressor(100 trees, random_state=0) with StandardScaler. Fills numeric NaNs with median and categorical NaNs with mode, then one-hot encodes via pd.get_dummies. Uses align(join='left', fill_value=0) for train/test column matching. Required 4 rewrites to fix: initial median() failing on mixed types, SalePrice in test num_cols, and SalePrice dropped by inner join align. -> 0.877",
    "root_4": "VotingRegressor ensemble of GradientBoostingRegressor and Ridge, with log1p target transform. Detailed manual missing value handling: LotFrontage with median, Alley/FireplaceQu/PoolQC/Fence/MiscFeature with 'None', BsmtQual/BsmtCond/GarageQual/GarageCond with 'TA', GarageType with 'Attchd', GarageFinish with 'Unf'. Uses ColumnTransformer with median+StandardScaler for numerics and mode+OneHotEncoder for categoricals. Best houseprice node. -> 0.917",
    "root_4_0": "Improvement on root_4: Single GradientBoostingRegressor(200 trees, lr=0.1, max_depth=3) replacing parent's VotingRegressor ensemble. Drops the log1p target transform and detailed per-column missing value handling from parent. Uses generic ColumnTransformer with median imputation + StandardScaler for numerics and mode + OneHotEncoder for categoricals. Simpler model with less feature engineering caused regression. -> 0.894",
    "root_4_0_0": "Improvement on root_4_0: Switches to RandomForestRegressor with GridSearchCV over n_estimators=[100,200], max_depth=[None,10,20], min_samples_split=[2,5] using 5-fold CV. Same ColumnTransformer pipeline as parent (median+StandardScaler for numerics, mode+OneHotEncoder for categoricals). First attempt failed due to missing OneHotEncoder import; fixed in second try. No log transform. -> 0.884"
  }
}
