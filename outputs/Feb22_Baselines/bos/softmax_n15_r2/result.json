{
  "task": "Battle of Sexes",
  "primary_metric": "Score",
  "higher_is_better": true,
  "selection_strategy": "softmax",
  "best_node_id": "root_2_1_0_0",
  "best_score": 1.4418400526046753,
  "baseline_score": 1.0226699113845825,
  "improvement": 0.4191701412200928,
  "total_nodes": 16,
  "elapsed_seconds": 410.1,
  "tree_shape": {
    "root": {
      "depth": 0,
      "num_children": 3,
      "score": 1.0226699113845825,
      "strategy": "Baseline (no model execution)"
    },
    "root_0": {
      "depth": 1,
      "num_children": 0,
      "score": 0.677340030670166,
      "strategy": "Adopt a mixed strategy where the row player chooses \"Coordination\" with probability 0.6 and \"Conflic"
    },
    "root_1": {
      "depth": 1,
      "num_children": 1,
      "score": 0.7879599928855896,
      "strategy": "Use a time-varying mixed strategy where the row player adjusts the probability of choosing \"Coordina"
    },
    "root_2": {
      "depth": 1,
      "num_children": 2,
      "score": 1.379159927368164,
      "strategy": "Apply a reinforcement learning-based adaptive strategy where the row player updates the probability "
    },
    "root_2_0": {
      "depth": 2,
      "num_children": 1,
      "score": 0.46153998374938965,
      "strategy": "Apply a contextual bandit strategy using a linear model with features derived from the opponent's pa"
    },
    "root_1_0": {
      "depth": 2,
      "num_children": 0,
      "score": 0.6676499843597412,
      "strategy": "Adopt a geometrically decaying mixed strategy where the probability of choosing \"Coordination\" start"
    },
    "root_2_1": {
      "depth": 2,
      "num_children": 3,
      "score": 0.8311299681663513,
      "strategy": "Implement a meta-strategy that alternates between two distinct behavioral modes: a high-payoff coord"
    },
    "root_2_1_0": {
      "depth": 3,
      "num_children": 1,
      "score": 1.3766601085662842,
      "strategy": "Implement a reinforcement learning-based adaptive strategy where the row player updates its action s"
    },
    "root_2_0_0": {
      "depth": 3,
      "num_children": 2,
      "score": 1.432479977607727,
      "strategy": "Apply a recurrent neural network (RNN) with a GRU layer to model the opponent's move sequence, using"
    },
    "root_2_1_1": {
      "depth": 3,
      "num_children": 0,
      "score": 0.8270799517631531,
      "strategy": "Implement a temporal difference learning (TD) strategy where the row player estimates the value of e"
    },
    "root_2_1_2": {
      "depth": 3,
      "num_children": 0,
      "score": 1.2293899059295654,
      "strategy": "Implement a quantum-inspired probabilistic strategy where the row player's action is determined by a"
    },
    "root_2_0_0_0": {
      "depth": 4,
      "num_children": 1,
      "score": 1.4378199577331543,
      "strategy": "Replace the GRU-based RNN with a Transformer model that processes the opponent's move history using "
    },
    "root_2_0_0_1": {
      "depth": 4,
      "num_children": 0,
      "score": 1.4377400875091553,
      "strategy": "Replace the time-aware embedding with a dynamic graph neural network (GNN) that models the opponent\u2019"
    },
    "root_2_0_0_0_0": {
      "depth": 5,
      "num_children": 1,
      "score": 1.239759922027588,
      "strategy": "Introduce a decision tree model that recursively partitions the opponent's move history based on pay"
    },
    "root_2_1_0_0": {
      "depth": 4,
      "num_children": 0,
      "score": 1.4418400526046753,
      "strategy": "Apply a temporal difference learning (TD) model with eligibility traces to predict future payoffs ba"
    },
    "root_2_0_0_0_0_0": {
      "depth": 6,
      "num_children": 0,
      "score": 1.3896499872207642,
      "strategy": "Apply a reinforcement learning-based policy that uses a Q-learning framework to update action values"
    }
  },
  "nodes": {
    "root": {
      "node_id": "root",
      "parent_id": null,
      "depth": 0,
      "strategy": "Baseline (no model execution)",
      "score": 1.0226699113845825,
      "actions_count": 0,
      "children": [
        "root_0",
        "root_1",
        "root_2"
      ],
      "error": null
    },
    "root_0": {
      "node_id": "root_0",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a mixed strategy where the row player chooses \"Coordination\" with probability 0.6 and \"Conflic",
      "score": 0.677340030670166,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_1": {
      "node_id": "root_1",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Use a time-varying mixed strategy where the row player adjusts the probability of choosing \"Coordina",
      "score": 0.7879599928855896,
      "actions_count": 4,
      "children": [
        "root_1_0"
      ],
      "error": null
    },
    "root_2": {
      "node_id": "root_2",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Apply a reinforcement learning-based adaptive strategy where the row player updates the probability ",
      "score": 1.379159927368164,
      "actions_count": 2,
      "children": [
        "root_2_0",
        "root_2_1"
      ],
      "error": null
    },
    "root_2_0": {
      "node_id": "root_2_0",
      "parent_id": "root_2",
      "depth": 2,
      "strategy": "Apply a contextual bandit strategy using a linear model with features derived from the opponent's pa",
      "score": 0.46153998374938965,
      "actions_count": 2,
      "children": [
        "root_2_0_0"
      ],
      "error": null
    },
    "root_1_0": {
      "node_id": "root_1_0",
      "parent_id": "root_1",
      "depth": 2,
      "strategy": "Adopt a geometrically decaying mixed strategy where the probability of choosing \"Coordination\" start",
      "score": 0.6676499843597412,
      "actions_count": 6,
      "children": [],
      "error": null
    },
    "root_2_1": {
      "node_id": "root_2_1",
      "parent_id": "root_2",
      "depth": 2,
      "strategy": "Implement a meta-strategy that alternates between two distinct behavioral modes: a high-payoff coord",
      "score": 0.8311299681663513,
      "actions_count": 2,
      "children": [
        "root_2_1_0",
        "root_2_1_1",
        "root_2_1_2"
      ],
      "error": null
    },
    "root_2_1_0": {
      "node_id": "root_2_1_0",
      "parent_id": "root_2_1",
      "depth": 3,
      "strategy": "Implement a reinforcement learning-based adaptive strategy where the row player updates its action s",
      "score": 1.3766601085662842,
      "actions_count": 2,
      "children": [
        "root_2_1_0_0"
      ],
      "error": null
    },
    "root_2_0_0": {
      "node_id": "root_2_0_0",
      "parent_id": "root_2_0",
      "depth": 3,
      "strategy": "Apply a recurrent neural network (RNN) with a GRU layer to model the opponent's move sequence, using",
      "score": 1.432479977607727,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0",
        "root_2_0_0_1"
      ],
      "error": null
    },
    "root_2_1_1": {
      "node_id": "root_2_1_1",
      "parent_id": "root_2_1",
      "depth": 3,
      "strategy": "Implement a temporal difference learning (TD) strategy where the row player estimates the value of e",
      "score": 0.8270799517631531,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_2_1_2": {
      "node_id": "root_2_1_2",
      "parent_id": "root_2_1",
      "depth": 3,
      "strategy": "Implement a quantum-inspired probabilistic strategy where the row player's action is determined by a",
      "score": 1.2293899059295654,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_2_0_0_0": {
      "node_id": "root_2_0_0_0",
      "parent_id": "root_2_0_0",
      "depth": 4,
      "strategy": "Replace the GRU-based RNN with a Transformer model that processes the opponent's move history using ",
      "score": 1.4378199577331543,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_1": {
      "node_id": "root_2_0_0_1",
      "parent_id": "root_2_0_0",
      "depth": 4,
      "strategy": "Replace the time-aware embedding with a dynamic graph neural network (GNN) that models the opponent\u2019",
      "score": 1.4377400875091553,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_2_0_0_0_0": {
      "node_id": "root_2_0_0_0_0",
      "parent_id": "root_2_0_0_0",
      "depth": 5,
      "strategy": "Introduce a decision tree model that recursively partitions the opponent's move history based on pay",
      "score": 1.239759922027588,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0_0_0"
      ],
      "error": null
    },
    "root_2_1_0_0": {
      "node_id": "root_2_1_0_0",
      "parent_id": "root_2_1_0",
      "depth": 4,
      "strategy": "Apply a temporal difference learning (TD) model with eligibility traces to predict future payoffs ba",
      "score": 1.4418400526046753,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_2_0_0_0_0_0": {
      "node_id": "root_2_0_0_0_0_0",
      "parent_id": "root_2_0_0_0_0",
      "depth": 6,
      "strategy": "Apply a reinforcement learning-based policy that uses a Q-learning framework to update action values",
      "score": 1.3896499872207642,
      "actions_count": 2,
      "children": [],
      "error": null
    }
  }
}