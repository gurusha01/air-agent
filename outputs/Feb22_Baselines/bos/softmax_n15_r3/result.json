{
  "task": "Battle of Sexes",
  "primary_metric": "Score",
  "higher_is_better": true,
  "selection_strategy": "softmax",
  "best_node_id": "root_2_0_0",
  "best_score": 1.3694599866867065,
  "baseline_score": 1.0226699113845825,
  "improvement": 0.346790075302124,
  "total_nodes": 16,
  "elapsed_seconds": 391.0,
  "tree_shape": {
    "root": {
      "depth": 0,
      "num_children": 3,
      "score": 1.0226699113845825,
      "strategy": "Baseline (no model execution)"
    },
    "root_0": {
      "depth": 1,
      "num_children": 2,
      "score": 0.9328100681304932,
      "strategy": "Adopt a mixed strategy where the row player randomizes between choosing \"Football\" and \"Opera\" with "
    },
    "root_1": {
      "depth": 1,
      "num_children": 1,
      "score": 0.7243099808692932,
      "strategy": "Use a context-dependent adaptive strategy where the row player adjusts their choice based on a simul"
    },
    "root_2": {
      "depth": 1,
      "num_children": 1,
      "score": 1.2993900775909424,
      "strategy": "Implement a time-series informed strategy where the row player observes the last three rounds' outco"
    },
    "root_2_0": {
      "depth": 2,
      "num_children": 2,
      "score": 1.1061100959777832,
      "strategy": "Implement a Markov chain-based state prediction strategy where the row player models the opponent's "
    },
    "root_2_0_0": {
      "depth": 3,
      "num_children": 1,
      "score": 1.3694599866867065,
      "strategy": "Implement a reinforcement learning-based strategy where the row player learns a policy using Q-learn"
    },
    "root_0_0": {
      "depth": 2,
      "num_children": 1,
      "score": 0.8231799602508545,
      "strategy": "Adopt a time-dependent mixed strategy where the row player adjusts the probability of choosing \"Foot"
    },
    "root_2_0_0_0": {
      "depth": 4,
      "num_children": 1,
      "score": 1.1020100116729736,
      "strategy": "Use a Markov Chain-based state transition model where the row player predicts the opponent's next mo"
    },
    "root_0_0_0": {
      "depth": 3,
      "num_children": 0,
      "score": 0.7010500431060791,
      "strategy": "Implement a noise-injected random walk strategy where the row player's probability of choosing \"Foot"
    },
    "root_2_0_1": {
      "depth": 3,
      "num_children": 1,
      "score": 1.1246000528335571,
      "strategy": "Apply a hidden Markov model (HMM) to infer the opponent's underlying preference states (e.g., \"Footb"
    },
    "root_0_1": {
      "depth": 2,
      "num_children": 0,
      "score": 0.8656400442123413,
      "strategy": "Implement a context-aware mixed strategy where the row player adjusts their choice based on the prev"
    },
    "root_1_0": {
      "depth": 2,
      "num_children": 0,
      "score": 0.8965099453926086,
      "strategy": "Implement a reinforcement learning-based strategy where the row player uses a Q-learning model to es"
    },
    "root_2_0_1_0": {
      "depth": 4,
      "num_children": 1,
      "score": 1.1264699697494507,
      "strategy": "Apply a reinforcement learning with curiosity-driven exploration (POMDP-based policy) where the row "
    },
    "root_2_0_0_0_0": {
      "depth": 5,
      "num_children": 1,
      "score": 1.2140198945999146,
      "strategy": "Apply a temporal difference learning (TD-Learning) policy where the row player updates its action se"
    },
    "root_2_0_1_0_0": {
      "depth": 5,
      "num_children": 0,
      "score": 1.1250500679016113,
      "strategy": "Implement a Bayesian game-theoretic approach where the row player models the opponent as a belief ov"
    },
    "root_2_0_0_0_0_0": {
      "depth": 6,
      "num_children": 0,
      "score": 1.0710099935531616,
      "strategy": "Apply a reinforcement learning policy using a Q-learning framework with episodic updates, where the "
    }
  },
  "nodes": {
    "root": {
      "node_id": "root",
      "parent_id": null,
      "depth": 0,
      "strategy": "Baseline (no model execution)",
      "score": 1.0226699113845825,
      "actions_count": 0,
      "children": [
        "root_0",
        "root_1",
        "root_2"
      ],
      "error": null
    },
    "root_0": {
      "node_id": "root_0",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Adopt a mixed strategy where the row player randomizes between choosing \"Football\" and \"Opera\" with ",
      "score": 0.9328100681304932,
      "actions_count": 2,
      "children": [
        "root_0_0",
        "root_0_1"
      ],
      "error": null
    },
    "root_1": {
      "node_id": "root_1",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Use a context-dependent adaptive strategy where the row player adjusts their choice based on a simul",
      "score": 0.7243099808692932,
      "actions_count": 2,
      "children": [
        "root_1_0"
      ],
      "error": null
    },
    "root_2": {
      "node_id": "root_2",
      "parent_id": "root",
      "depth": 1,
      "strategy": "Implement a time-series informed strategy where the row player observes the last three rounds' outco",
      "score": 1.2993900775909424,
      "actions_count": 2,
      "children": [
        "root_2_0"
      ],
      "error": null
    },
    "root_2_0": {
      "node_id": "root_2_0",
      "parent_id": "root_2",
      "depth": 2,
      "strategy": "Implement a Markov chain-based state prediction strategy where the row player models the opponent's ",
      "score": 1.1061100959777832,
      "actions_count": 2,
      "children": [
        "root_2_0_0",
        "root_2_0_1"
      ],
      "error": null
    },
    "root_2_0_0": {
      "node_id": "root_2_0_0",
      "parent_id": "root_2_0",
      "depth": 3,
      "strategy": "Implement a reinforcement learning-based strategy where the row player learns a policy using Q-learn",
      "score": 1.3694599866867065,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0"
      ],
      "error": null
    },
    "root_0_0": {
      "node_id": "root_0_0",
      "parent_id": "root_0",
      "depth": 2,
      "strategy": "Adopt a time-dependent mixed strategy where the row player adjusts the probability of choosing \"Foot",
      "score": 0.8231799602508545,
      "actions_count": 2,
      "children": [
        "root_0_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0": {
      "node_id": "root_2_0_0_0",
      "parent_id": "root_2_0_0",
      "depth": 4,
      "strategy": "Use a Markov Chain-based state transition model where the row player predicts the opponent's next mo",
      "score": 1.1020100116729736,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0_0"
      ],
      "error": null
    },
    "root_0_0_0": {
      "node_id": "root_0_0_0",
      "parent_id": "root_0_0",
      "depth": 3,
      "strategy": "Implement a noise-injected random walk strategy where the row player's probability of choosing \"Foot",
      "score": 0.7010500431060791,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_2_0_1": {
      "node_id": "root_2_0_1",
      "parent_id": "root_2_0",
      "depth": 3,
      "strategy": "Apply a hidden Markov model (HMM) to infer the opponent's underlying preference states (e.g., \"Footb",
      "score": 1.1246000528335571,
      "actions_count": 2,
      "children": [
        "root_2_0_1_0"
      ],
      "error": null
    },
    "root_0_1": {
      "node_id": "root_0_1",
      "parent_id": "root_0",
      "depth": 2,
      "strategy": "Implement a context-aware mixed strategy where the row player adjusts their choice based on the prev",
      "score": 0.8656400442123413,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_1_0": {
      "node_id": "root_1_0",
      "parent_id": "root_1",
      "depth": 2,
      "strategy": "Implement a reinforcement learning-based strategy where the row player uses a Q-learning model to es",
      "score": 0.8965099453926086,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_2_0_1_0": {
      "node_id": "root_2_0_1_0",
      "parent_id": "root_2_0_1",
      "depth": 4,
      "strategy": "Apply a reinforcement learning with curiosity-driven exploration (POMDP-based policy) where the row ",
      "score": 1.1264699697494507,
      "actions_count": 2,
      "children": [
        "root_2_0_1_0_0"
      ],
      "error": null
    },
    "root_2_0_0_0_0": {
      "node_id": "root_2_0_0_0_0",
      "parent_id": "root_2_0_0_0",
      "depth": 5,
      "strategy": "Apply a temporal difference learning (TD-Learning) policy where the row player updates its action se",
      "score": 1.2140198945999146,
      "actions_count": 2,
      "children": [
        "root_2_0_0_0_0_0"
      ],
      "error": null
    },
    "root_2_0_1_0_0": {
      "node_id": "root_2_0_1_0_0",
      "parent_id": "root_2_0_1_0",
      "depth": 5,
      "strategy": "Implement a Bayesian game-theoretic approach where the row player models the opponent as a belief ov",
      "score": 1.1250500679016113,
      "actions_count": 2,
      "children": [],
      "error": null
    },
    "root_2_0_0_0_0_0": {
      "node_id": "root_2_0_0_0_0_0",
      "parent_id": "root_2_0_0_0_0",
      "depth": 6,
      "strategy": "Apply a reinforcement learning policy using a Q-learning framework with episodic updates, where the ",
      "score": 1.0710099935531616,
      "actions_count": 2,
      "children": [],
      "error": null
    }
  }
}