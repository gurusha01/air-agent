# MLGym Training Config - Experiment 1.6
#
# Continuous GRPO-style reward in [-1, 1]
# reward = (improvement * 5 - 0.5) + format_penalty
#
# Reward scheme:
# -  0.0 for correct tool call
# - -0.1 for incorrect tool call
# - +1.0 for accuracy >= 0.93 at episode end
# - -0.2 for mediocre improvement (above baseline but < 0.93)
# - -1.0 for no improvement at episode end
#
# Run with:
#   cd /home/ubuntu/MLScientist/MLGym
#   source /home/ubuntu/MLScientist/air-agent/.venv/bin/activate
#   uv run --project /home/ubuntu/MLScientist/air-agent rl @ /home/ubuntu/MLScientist/air-agent/configs/mlgym/rl_full.toml

# GPU allocation:
# - GPU 0: vLLM inference server
# - GPUs 2-6: FSDP trainer (5 GPUs)
# - GPU 7: MLGym Docker container
inference_gpu_ids = [0]
trainer_gpu_ids = [2, 3, 4, 5, 6]

# Training duration
# 1 task × 10 examples = 10 examples
# batch_size=16, rollouts_per_example=8 → 2 examples per step
# 1 epoch = 10/2 = 5 steps
# 40 epochs = 200 steps
max_steps = 200
seq_len = 32768  # Increased for multi-turn conversations (Qwen3 supports 256K)

[ckpt]
interval = 20

[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

[wandb]
project = "mlgym-rl"
name = "exp1.6-titanic-0208"

# ═══════════════════════════════════════════════════════════════
# TRAINER CONFIG
# ═══════════════════════════════════════════════════════════════
[trainer.model]
impl = "auto"

[trainer.model.ac]
freq = 1

[trainer.model.lora]
rank = 16
alpha = 32

[trainer.optim]
lr = 1e-5

# ═══════════════════════════════════════════════════════════════
# ORCHESTRATOR CONFIG
# ═══════════════════════════════════════════════════════════════
[orchestrator]
batch_size = 16                     # Trajectories per training step
rollouts_per_example = 8            # 8 rollouts per prompt (in parallel)
trajectory_strategy = "interleaved"
workers_per_env = 2

[orchestrator.log]
env_worker_logs = true
level = "INFO"

[orchestrator.sampling]
max_tokens = 2048
temperature = 0.8

# Single task: Titanic (tabular classification)
[[orchestrator.env]]
id = "air_mlgym"
name = "titanic"
args = { task_configs = ["titanic.yaml"], max_turns = 20, num_examples_per_task = 10, env_gpu = "7", save_trajectories = true, trajectory_dir = "outputs/trajectories_exp1.6" }

# ═══════════════════════════════════════════════════════════════
# INFERENCE CONFIG
# ═══════════════════════════════════════════════════════════════
[inference.model]
max_model_len = 32768  # Increased for multi-turn conversations (Qwen3 supports 256K)
