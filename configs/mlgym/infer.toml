# MLGym Inference Server Configuration for Prime-RL
#
# This config sets up the vLLM inference server for MLGym training.

# Model configuration
model = "Qwen/Qwen3-4B-Instruct-2507"
trust_remote_code = true

# Server configuration
host = "0.0.0.0"
port = 8000

# vLLM settings
max_model_len = 8192
tensor_parallel_size = 1
gpu_memory_utilization = 0.9

# LoRA support (for serving trained adapters)
enable_lora = true
max_lora_rank = 64

# Weights directory (for loading from trainer checkpoints)
# weights_dir = "outputs/mlgym_run/weights"

# Logging
# log_level = "info"
